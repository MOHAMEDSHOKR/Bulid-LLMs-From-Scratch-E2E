{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5538zuBsDFnCPlCWIHUZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOHAMEDSHOKR/LLMs-From-Scratch-/blob/main/LLMs%20\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!get clone https://github.com/MOHAMEDSHOKR/LLMs-From-Scratch-.git\n",
        "import urllib.request\n",
        "import re\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "\"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "     raw_text = f.read()\n",
        "# new_text = \"\"\" It's the last he painted, you know,\"\n",
        "# Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])\n",
        "\n",
        "preprocced = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocced = [item.strip() for item in preprocced if item.strip()]\n",
        "\n",
        "print(len(preprocced))\n",
        "print(preprocced[:30])\n",
        "\n",
        "all_words = sorted(set(preprocced))\n",
        "\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)\n",
        "\n",
        "# now we will impelment endcod method to converrt input text into tokens\n",
        "#convert Input Text in to tokens then into vocabualry(added for every unique token, token ID)\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "     print(item)\n",
        "\n",
        "     if i >= 50:\n",
        "\n",
        "      break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JehRD61KhEH",
        "outputId": "ddb9339d-20a2-4a18-9359-af2be713dc39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: get: command not found\n",
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
            "4690\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
            "1130\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-bWCwQ6O-Q3",
        "outputId": "70af96dc-4af6-4167-ad47-b40e3a3148fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'Ah': 12, 'Among': 13, 'And': 14, 'Are': 15, 'Arrt': 16, 'As': 17, 'At': 18, 'Be': 19, 'Begin': 20, 'Burlington': 21, 'But': 22, 'By': 23, 'Carlo': 24, 'Chicago': 25, 'Claude': 26, 'Come': 27, 'Croft': 28, 'Destroyed': 29, 'Devonshire': 30, 'Don': 31, 'Dubarry': 32, 'Emperors': 33, 'Florence': 34, 'For': 35, 'Gallery': 36, 'Gideon': 37, 'Gisburn': 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, 'Grindles': 43, 'HAD': 44, 'Had': 45, 'Hang': 46, 'Has': 47, 'He': 48, 'Her': 49, 'Hermia': 50, 'His': 51, 'How': 52, 'I': 53, 'If': 54, 'In': 55, 'It': 56, 'Jack': 57, 'Jove': 58, 'Just': 59, 'Lord': 60, 'Made': 61, 'Miss': 62, 'Money': 63, 'Monte': 64, 'Moon-dancers': 65, 'Mr': 66, 'Mrs': 67, 'My': 68, 'Never': 69, 'No': 70, 'Now': 71, 'Nutley': 72, 'Of': 73, 'Oh': 74, 'On': 75, 'Once': 76, 'Only': 77, 'Or': 78, 'Perhaps': 79, 'Poor': 80, 'Professional': 81, 'Renaissance': 82, 'Rickham': 83, 'Riviera': 84, 'Rome': 85, 'Russian': 86, 'Sevres': 87, 'She': 88, 'Stroud': 89, 'Strouds': 90, 'Suddenly': 91, 'That': 92, 'The': 93, 'Then': 94, 'There': 95, 'They': 96, 'This': 97, 'Those': 98, 'Though': 99, 'Thwing': 100, 'Thwings': 101, 'To': 102, 'Usually': 103, 'Venetian': 104, 'Victor': 105, 'Was': 106, 'We': 107, 'Well': 108, 'What': 109, 'When': 110, 'Why': 111, 'Yes': 112, 'You': 113, '_': 114, 'a': 115, 'abdication': 116, 'able': 117, 'about': 118, 'above': 119, 'abruptly': 120, 'absolute': 121, 'absorbed': 122, 'absurdity': 123, 'academic': 124, 'accuse': 125, 'accustomed': 126, 'across': 127, 'activity': 128, 'add': 129, 'added': 130, 'admirers': 131, 'adopted': 132, 'adulation': 133, 'advance': 134, 'aesthetic': 135, 'affect': 136, 'afraid': 137, 'after': 138, 'afterward': 139, 'again': 140, 'ago': 141, 'ah': 142, 'air': 143, 'alive': 144, 'all': 145, 'almost': 146, 'alone': 147, 'along': 148, 'always': 149, 'am': 150, 'amazement': 151, 'amid': 152, 'among': 153, 'amplest': 154, 'amusing': 155, 'an': 156, 'and': 157, 'another': 158, 'answer': 159, 'answered': 160, 'any': 161, 'anything': 162, 'anywhere': 163, 'apparent': 164, 'apparently': 165, 'appearance': 166, 'appeared': 167, 'appointed': 168, 'are': 169, 'arm': 170, 'arm-chair': 171, 'arm-chairs': 172, 'arms': 173, 'art': 174, 'articles': 175, 'artist': 176, 'as': 177, 'aside': 178, 'asked': 179, 'at': 180, 'atmosphere': 181, 'atom': 182, 'attack': 183, 'attention': 184, 'attitude': 185, 'audacities': 186, 'away': 187, 'awful': 188, 'axioms': 189, 'azaleas': 190, 'back': 191, 'background': 192, 'balance': 193, 'balancing': 194, 'balustraded': 195, 'basking': 196, 'bath-rooms': 197, 'be': 198, 'beaming': 199, 'bean-stalk': 200, 'bear': 201, 'beard': 202, 'beauty': 203, 'became': 204, 'because': 205, 'becoming': 206, 'bed': 207, 'been': 208, 'before': 209, 'began': 210, 'begun': 211, 'behind': 212, 'being': 213, 'believed': 214, 'beneath': 215, 'bespoke': 216, 'better': 217, 'between': 218, 'big': 219, 'bits': 220, 'bitterness': 221, 'blocked': 222, 'born': 223, 'borne': 224, 'boudoir': 225, 'bravura': 226, 'break': 227, 'breaking': 228, 'breathing': 229, 'bric-a-brac': 230, 'briefly': 231, 'brings': 232, 'bronzes': 233, 'brought': 234, 'brown': 235, 'brush': 236, 'bull': 237, 'business': 238, 'but': 239, 'buying': 240, 'by': 241, 'called': 242, 'came': 243, 'can': 244, 'canvas': 245, 'canvases': 246, 'cards': 247, 'care': 248, 'career': 249, 'caught': 250, 'central': 251, 'chair': 252, 'chap': 253, 'characteristic': 254, 'charming': 255, 'cheap': 256, 'check': 257, 'cheeks': 258, 'chest': 259, 'chimney-piece': 260, 'chucked': 261, 'cigar': 262, 'cigarette': 263, 'cigars': 264, 'circulation': 265, 'circumstance': 266, 'circus-clown': 267, 'claimed': 268, 'clasping': 269, 'clear': 270, 'cleverer': 271, 'close': 272, 'clue': 273, 'coat': 274, 'collapsed': 275, 'colour': 276, 'come': 277, 'comfortable': 278, 'coming': 279, 'companion': 280, 'compared': 281, 'complex': 282, 'confident': 283, 'congesting': 284, 'conjugal': 285, 'constraint': 286, 'consummate': 287, 'contended': 288, 'continued': 289, 'corner': 290, 'corrected': 291, 'could': 292, 'couldn': 293, 'count': 294, 'countenance': 295, 'couple': 296, 'course': 297, 'covered': 298, 'craft': 299, 'cried': 300, 'crossed': 301, 'crowned': 302, 'crumbled': 303, 'cry': 304, 'cured': 305, 'curiosity': 306, 'curious': 307, 'current': 308, 'curtains': 309, 'd': 310, 'dabble': 311, 'damask': 312, 'dark': 313, 'dashed': 314, 'day': 315, 'days': 316, 'dead': 317, 'deadening': 318, 'dear': 319, 'deep': 320, 'deerhound': 321, 'degree': 322, 'delicate': 323, 'demand': 324, 'denied': 325, 'deploring': 326, 'deprecating': 327, 'deprecatingly': 328, 'desire': 329, 'destroyed': 330, 'destruction': 331, 'desultory': 332, 'detail': 333, 'diagnosis': 334, 'did': 335, 'didn': 336, 'died': 337, 'dim': 338, 'dimmest': 339, 'dingy': 340, 'dining-room': 341, 'disarming': 342, 'discovery': 343, 'discrimination': 344, 'discussion': 345, 'disdain': 346, 'disdained': 347, 'disease': 348, 'disguised': 349, 'display': 350, 'dissatisfied': 351, 'distinguished': 352, 'distract': 353, 'divert': 354, 'do': 355, 'doesn': 356, 'doing': 357, 'domestic': 358, 'don': 359, 'done': 360, 'donkey': 361, 'down': 362, 'dozen': 363, 'dragged': 364, 'drawing-room': 365, 'drawing-rooms': 366, 'drawn': 367, 'dress-closets': 368, 'drew': 369, 'dropped': 370, 'each': 371, 'earth': 372, 'ease': 373, 'easel': 374, 'easy': 375, 'echoed': 376, 'economy': 377, 'effect': 378, 'effects': 379, 'efforts': 380, 'egregious': 381, 'eighteenth-century': 382, 'elbow': 383, 'elegant': 384, 'else': 385, 'embarrassed': 386, 'enabled': 387, 'end': 388, 'endless': 389, 'enjoy': 390, 'enlightenment': 391, 'enough': 392, 'ensuing': 393, 'equally': 394, 'equanimity': 395, 'escape': 396, 'established': 397, 'etching': 398, 'even': 399, 'event': 400, 'ever': 401, 'everlasting': 402, 'every': 403, 'exasperated': 404, 'except': 405, 'excuse': 406, 'excusing': 407, 'existed': 408, 'expected': 409, 'exquisite': 410, 'exquisitely': 411, 'extenuation': 412, 'exterminating': 413, 'extracting': 414, 'eye': 415, 'eyebrows': 416, 'eyes': 417, 'face': 418, 'faces': 419, 'fact': 420, 'faded': 421, 'failed': 422, 'failure': 423, 'fair': 424, 'faith': 425, 'false': 426, 'familiar': 427, 'famille-verte': 428, 'fancy': 429, 'fashionable': 430, 'fate': 431, 'feather': 432, 'feet': 433, 'fell': 434, 'fellow': 435, 'felt': 436, 'few': 437, 'fewer': 438, 'finality': 439, 'find': 440, 'fingers': 441, 'first': 442, 'fit': 443, 'fitting': 444, 'five': 445, 'flash': 446, 'flashed': 447, 'florid': 448, 'flowers': 449, 'fluently': 450, 'flung': 451, 'follow': 452, 'followed': 453, 'fond': 454, 'footstep': 455, 'for': 456, 'forced': 457, 'forcing': 458, 'forehead': 459, 'foreign': 460, 'foreseen': 461, 'forgive': 462, 'forgotten': 463, 'form': 464, 'formed': 465, 'forming': 466, 'forward': 467, 'fostered': 468, 'found': 469, 'foundations': 470, 'fragment': 471, 'fragments': 472, 'frame': 473, 'frames': 474, 'frequently': 475, 'friend': 476, 'from': 477, 'full': 478, 'fullest': 479, 'furiously': 480, 'furrowed': 481, 'garlanded': 482, 'garlands': 483, 'gave': 484, 'genial': 485, 'genius': 486, 'gesture': 487, 'get': 488, 'getting': 489, 'give': 490, 'given': 491, 'glad': 492, 'glanced': 493, 'glimpse': 494, 'gloried': 495, 'glory': 496, 'go': 497, 'going': 498, 'gone': 499, 'good': 500, 'good-breeding': 501, 'good-humoured': 502, 'got': 503, 'grace': 504, 'gradually': 505, 'gray': 506, 'grayish': 507, 'great': 508, 'greatest': 509, 'greatness': 510, 'grew': 511, 'groping': 512, 'growing': 513, 'had': 514, 'hadn': 515, 'hair': 516, 'half': 517, 'half-light': 518, 'half-mechanically': 519, 'hall': 520, 'hand': 521, 'hands': 522, 'handsome': 523, 'hanging': 524, 'happen': 525, 'happened': 526, 'hard': 527, 'hardly': 528, 'has': 529, 'have': 530, 'haven': 531, 'having': 532, 'he': 533, 'head': 534, 'hear': 535, 'heard': 536, 'heart': 537, 'height': 538, 'her': 539, 'here': 540, 'hermit': 541, 'herself': 542, 'hesitations': 543, 'hide': 544, 'high': 545, 'him': 546, 'himself': 547, 'hint': 548, 'his': 549, 'history': 550, 'holding': 551, 'home': 552, 'honour': 553, 'hooded': 554, 'hostess': 555, 'hot-house': 556, 'hour': 557, 'hours': 558, 'house': 559, 'how': 560, 'hung': 561, 'husband': 562, 'idea': 563, 'idle': 564, 'idling': 565, 'if': 566, 'immediately': 567, 'in': 568, 'incense': 569, 'indifferent': 570, 'inevitable': 571, 'inevitably': 572, 'inflexible': 573, 'insensible': 574, 'insignificant': 575, 'instinctively': 576, 'instructive': 577, 'interesting': 578, 'into': 579, 'ironic': 580, 'irony': 581, 'irrelevance': 582, 'irrevocable': 583, 'is': 584, 'it': 585, 'its': 586, 'itself': 587, 'jardiniere': 588, 'jealousy': 589, 'just': 590, 'keep': 591, 'kept': 592, 'kind': 593, 'knees': 594, 'knew': 595, 'know': 596, 'known': 597, 'laid': 598, 'lair': 599, 'landing': 600, 'language': 601, 'last': 602, 'late': 603, 'later': 604, 'latter': 605, 'laugh': 606, 'laughed': 607, 'lay': 608, 'leading': 609, 'lean': 610, 'learned': 611, 'least': 612, 'leathery': 613, 'leave': 614, 'led': 615, 'left': 616, 'leisure': 617, 'lends': 618, 'lent': 619, 'let': 620, 'lies': 621, 'life': 622, 'life-likeness': 623, 'lift': 624, 'lifted': 625, 'light': 626, 'lightly': 627, 'like': 628, 'liked': 629, 'line': 630, 'lines': 631, 'lingered': 632, 'lips': 633, 'lit': 634, 'little': 635, 'live': 636, 'll': 637, 'loathing': 638, 'long': 639, 'longed': 640, 'longer': 641, 'look': 642, 'looked': 643, 'looking': 644, 'lose': 645, 'loss': 646, 'lounging': 647, 'lovely': 648, 'lucky': 649, 'lump': 650, 'luncheon-table': 651, 'luxury': 652, 'lying': 653, 'made': 654, 'make': 655, 'man': 656, 'manage': 657, 'managed': 658, 'mantel-piece': 659, 'marble': 660, 'married': 661, 'may': 662, 'me': 663, 'meant': 664, 'mediocrity': 665, 'medium': 666, 'mentioned': 667, 'mere': 668, 'merely': 669, 'met': 670, 'might': 671, 'mighty': 672, 'millionaire': 673, 'mine': 674, 'minute': 675, 'minutes': 676, 'mirrors': 677, 'modest': 678, 'modesty': 679, 'moment': 680, 'money': 681, 'monumental': 682, 'mood': 683, 'morbidly': 684, 'more': 685, 'most': 686, 'mourn': 687, 'mourned': 688, 'moustache': 689, 'moved': 690, 'much': 691, 'muddling': 692, 'multiplied': 693, 'murmur': 694, 'muscles': 695, 'must': 696, 'my': 697, 'myself': 698, 'mysterious': 699, 'naive': 700, 'near': 701, 'nearly': 702, 'negatived': 703, 'nervous': 704, 'nervousness': 705, 'neutral': 706, 'never': 707, 'next': 708, 'no': 709, 'none': 710, 'not': 711, 'note': 712, 'nothing': 713, 'now': 714, 'nymphs': 715, 'oak': 716, 'obituary': 717, 'object': 718, 'objects': 719, 'occurred': 720, 'oddly': 721, 'of': 722, 'off': 723, 'often': 724, 'oh': 725, 'old': 726, 'on': 727, 'once': 728, 'one': 729, 'ones': 730, 'only': 731, 'onto': 732, 'open': 733, 'or': 734, 'other': 735, 'our': 736, 'ourselves': 737, 'out': 738, 'outline': 739, 'oval': 740, 'over': 741, 'own': 742, 'packed': 743, 'paid': 744, 'paint': 745, 'painted': 746, 'painter': 747, 'painting': 748, 'pale': 749, 'paled': 750, 'palm-trees': 751, 'panel': 752, 'panelling': 753, 'pardonable': 754, 'pardoned': 755, 'part': 756, 'passages': 757, 'passing': 758, 'past': 759, 'pastels': 760, 'pathos': 761, 'patient': 762, 'people': 763, 'perceptible': 764, 'perfect': 765, 'persistence': 766, 'persuasively': 767, 'phrase': 768, 'picture': 769, 'pictures': 770, 'pines': 771, 'pink': 772, 'place': 773, 'placed': 774, 'plain': 775, 'platitudes': 776, 'pleased': 777, 'pockets': 778, 'point': 779, 'poised': 780, 'poor': 781, 'portrait': 782, 'posing': 783, 'possessed': 784, 'poverty': 785, 'predicted': 786, 'preliminary': 787, 'presenting': 788, 'prestidigitation': 789, 'pretty': 790, 'previous': 791, 'price': 792, 'pride': 793, 'princely': 794, 'prism': 795, 'problem': 796, 'proclaiming': 797, 'prodigious': 798, 'profusion': 799, 'protest': 800, 'prove': 801, 'public': 802, 'purblind': 803, 'purely': 804, 'pushed': 805, 'put': 806, 'qualities': 807, 'quality': 808, 'queerly': 809, 'question': 810, 'quickly': 811, 'quietly': 812, 'quite': 813, 'quote': 814, 'rain': 815, 'raised': 816, 'random': 817, 'rather': 818, 're': 819, 'real': 820, 'really': 821, 'reared': 822, 'reason': 823, 'reassurance': 824, 'recovering': 825, 'recreated': 826, 'reflected': 827, 'reflection': 828, 'regrets': 829, 'relatively': 830, 'remained': 831, 'remember': 832, 'reminded': 833, 'repeating': 834, 'represented': 835, 'reproduction': 836, 'resented': 837, 'resolve': 838, 'resources': 839, 'rest': 840, 'rich': 841, 'ridiculous': 842, 'robbed': 843, 'romantic': 844, 'room': 845, 'rose': 846, 'rs': 847, 'rule': 848, 'run': 849, 's': 850, 'said': 851, 'same': 852, 'satisfaction': 853, 'savour': 854, 'saw': 855, 'say': 856, 'saying': 857, 'says': 858, 'scorn': 859, 'scornful': 860, 'secret': 861, 'see': 862, 'seemed': 863, 'seen': 864, 'self-confident': 865, 'send': 866, 'sensation': 867, 'sensitive': 868, 'sent': 869, 'serious': 870, 'set': 871, 'sex': 872, 'shade': 873, 'shaking': 874, 'shall': 875, 'she': 876, 'shirked': 877, 'short': 878, 'should': 879, 'shoulder': 880, 'shoulders': 881, 'show': 882, 'showed': 883, 'showy': 884, 'shrug': 885, 'shrugged': 886, 'sight': 887, 'sign': 888, 'silent': 889, 'silver': 890, 'similar': 891, 'simpleton': 892, 'simplifications': 893, 'simply': 894, 'since': 895, 'single': 896, 'sitter': 897, 'sitters': 898, 'sketch': 899, 'skill': 900, 'slight': 901, 'slightly': 902, 'slowly': 903, 'small': 904, 'smile': 905, 'smiling': 906, 'sneer': 907, 'so': 908, 'solace': 909, 'some': 910, 'somebody': 911, 'something': 912, 'spacious': 913, 'spaniel': 914, 'speaking-tubes': 915, 'speculations': 916, 'spite': 917, 'splash': 918, 'square': 919, 'stairs': 920, 'stammer': 921, 'stand': 922, 'standing': 923, 'started': 924, 'stay': 925, 'still': 926, 'stocked': 927, 'stood': 928, 'stopped': 929, 'stopping': 930, 'straddling': 931, 'straight': 932, 'strain': 933, 'straining': 934, 'strange': 935, 'straw': 936, 'stream': 937, 'stroke': 938, 'strokes': 939, 'strolled': 940, 'strongest': 941, 'strongly': 942, 'struck': 943, 'studio': 944, 'stuff': 945, 'subject': 946, 'substantial': 947, 'suburban': 948, 'such': 949, 'suddenly': 950, 'suffered': 951, 'sugar': 952, 'suggested': 953, 'sunburn': 954, 'sunburnt': 955, 'sunlit': 956, 'superb': 957, 'sure': 958, 'surest': 959, 'surface': 960, 'surprise': 961, 'surprised': 962, 'surrounded': 963, 'suspected': 964, 'sweetly': 965, 'sweetness': 966, 'swelling': 967, 'swept': 968, 'swum': 969, 't': 970, 'table': 971, 'take': 972, 'taken': 973, 'talking': 974, 'tea': 975, 'tears': 976, 'technicalities': 977, 'technique': 978, 'tell': 979, 'tells': 980, 'tempting': 981, 'terra-cotta': 982, 'terrace': 983, 'terraces': 984, 'terribly': 985, 'than': 986, 'that': 987, 'the': 988, 'their': 989, 'them': 990, 'then': 991, 'there': 992, 'therefore': 993, 'they': 994, 'thin': 995, 'thing': 996, 'things': 997, 'think': 998, 'this': 999, 'thither': 1000, 'those': 1001, 'though': 1002, 'thought': 1003, 'three': 1004, 'threshold': 1005, 'threw': 1006, 'through': 1007, 'throwing': 1008, 'tie': 1009, 'till': 1010, 'time': 1011, 'timorously': 1012, 'tinge': 1013, 'tips': 1014, 'tired': 1015, 'to': 1016, 'told': 1017, 'tone': 1018, 'tones': 1019, 'too': 1020, 'took': 1021, 'tottering': 1022, 'touched': 1023, 'toward': 1024, 'trace': 1025, 'trade': 1026, 'transmute': 1027, 'traps': 1028, 'travelled': 1029, 'tribute': 1030, 'tributes': 1031, 'tricks': 1032, 'tried': 1033, 'trouser-presses': 1034, 'true': 1035, 'truth': 1036, 'turned': 1037, 'twenty': 1038, 'twenty-four': 1039, 'twice': 1040, 'twirling': 1041, 'unaccountable': 1042, 'uncertain': 1043, 'under': 1044, 'underlay': 1045, 'underneath': 1046, 'understand': 1047, 'unexpected': 1048, 'untouched': 1049, 'unusual': 1050, 'up': 1051, 'up-stream': 1052, 'upon': 1053, 'upset': 1054, 'upstairs': 1055, 'us': 1056, 'used': 1057, 'usual': 1058, 'value': 1059, 'varnishing': 1060, 'vases': 1061, 've': 1062, 'veins': 1063, 'velveteen': 1064, 'very': 1065, 'villa': 1066, 'vindicated': 1067, 'virtuosity': 1068, 'vista': 1069, 'vocation': 1070, 'voice': 1071, 'wall': 1072, 'wander': 1073, 'want': 1074, 'wanted': 1075, 'wants': 1076, 'was': 1077, 'wasn': 1078, 'watched': 1079, 'watching': 1080, 'water-colour': 1081, 'waves': 1082, 'way': 1083, 'weekly': 1084, 'weeks': 1085, 'welcome': 1086, 'went': 1087, 'were': 1088, 'what': 1089, 'when': 1090, 'whenever': 1091, 'where': 1092, 'which': 1093, 'while': 1094, 'white': 1095, 'white-panelled': 1096, 'who': 1097, 'whole': 1098, 'whom': 1099, 'why': 1100, 'wide': 1101, 'widow': 1102, 'wife': 1103, 'wild': 1104, 'wincing': 1105, 'window-curtains': 1106, 'wish': 1107, 'with': 1108, 'without': 1109, 'wits': 1110, 'woman': 1111, 'women': 1112, 'won': 1113, 'wonder': 1114, 'wondered': 1115, 'word': 1116, 'work': 1117, 'working': 1118, 'worth': 1119, 'would': 1120, 'wouldn': 1121, 'year': 1122, 'years': 1123, 'yellow': 1124, 'yet': 1125, 'you': 1126, 'younger': 1127, 'your': 1128, 'yourself': 1129}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 2.3 Implementing a simple text tokenizer\n",
        "\"\"\" Stores the vocabulary as a class attribute for\n",
        "access in the encode and decode methods \"\"\"\n",
        "\n",
        "class SimpleTokenizerV1:\n",
        "\n",
        "    def __init__(self, vocab) :\n",
        "        self.str_into_int = vocab\n",
        "        self.int_into_str = {i:s for s, i in vocab.items()} #Creates an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "    # process Input Text into text IDs\n",
        "    def encode(self, text):\n",
        "\n",
        "      preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "      preprocessed = [\n",
        "                item.strip() for item in preprocessed if item.strip()]\n",
        "      ids = [self.str_into_int[s] for s in preprocced]\n",
        "\n",
        "      return ids\n",
        "\n",
        "    #convert tokens IDs into text\n",
        "    def decode (self, ids):\n",
        "\n",
        "      text = \" \".join(self.int_into_str[i] for i in ids)\n",
        "\n",
        "      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # remove spaces before the specified punctuation\n",
        "\n",
        "      return text\n",
        "\n",
        "new_text = \"\"\" It's the last he painted, you know,\"\n",
        "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "\n",
        "ids = tokenizer.encode(new_text)\n",
        "\n",
        "print(ids)\n",
        "\n",
        "decoded_text = tokenizer.decode(ids)\n",
        "\n",
        "print(decoded_text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFprxIk-LtO",
        "outputId": "875c5644-0907-4921-df58-756891f5a202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 1, 13, 15, 9, 7, 10, 2, 17, 8, 2, 0, 6, 3, 4, 14, 16, 11, 12, 3]\n",
            "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 2.4 A simple text tokenizer that handles unknown words\n",
        "class SimpleTokenizerV2:\n",
        "\n",
        "  def __init__(self, vocab) :\n",
        "\n",
        "     self.str_into_int = vocab\n",
        "\n",
        "          # Add the <|unk|> token to the vocabulary if it's not present\n",
        "     if \"|<unk>|\" not in self.str_into_int:\n",
        "      self.str_into_int[\"|<unk>|\"] = len(self.str_into_int)\n",
        "\n",
        "     self.int_into_str = {i:s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "    preprocessed = [item if item in self.str_into_int else \"<|unk|>\" for item in preprocessed]\n",
        "\n",
        "    ids = [self.str_into_int[s] for s in preprocced]\n",
        "    return ids\n",
        "\n",
        "  def decode (self, ids):\n",
        "\n",
        "    text = \" \".join(self.int_into_str[i] for i in ids)\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \"|<endoftext>|\".join([text1, text2])\n",
        "\n",
        "print(text)\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVGRkSahjvbB",
        "outputId": "769c2f34-c8b0-4adc-8feb-92dd884c0911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea?|<endoftext>|In the sunlit terraces of the palace.\n",
            "[2, 0, 3, 6, 4, 5, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" we will used advanced technice to decode and encode Input Text by using tokenizer from tiktoken library\n",
        "\n",
        "     tiktoken is library from Openai based on concept callled Byte pair Encoding (BPE) that used to train LLMs like GPT-2, GPT-3\n",
        "\"\"\"\n",
        "!pip install tiktoken\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "\n",
        "print(version(\"tiktoken\"))\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "text = (\n",
        "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "\"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)\n",
        "\n",
        "new_text = \"Akwirw ier\"\n",
        "integ = tokenizer.encode(new_text)\n",
        "print(integ)\n",
        "\n",
        "string = tokenizer.decode(integ)\n",
        "print(string)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQFXGVmaLqAn",
        "outputId": "bdaa4e34-bc2a-4d7b-a407-7144438a615a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "0.8.0\n",
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n",
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import urllib.request\n",
        "import re\n",
        "import tiktoken\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "\"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "     raw_text = f.read()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "enc = tokenizer.encode(raw_text)\n",
        "print(len(enc))\n",
        "en_sample = enc[50:]\n",
        "\n",
        "\"\"\"create two variables, x and y, where x contains the input\n",
        "tokens and y contains the targets, which are the inputs shifted by 1:\"\"\"\n",
        "\n",
        "context_size = 10\n",
        "x = en_sample[:context_size] # x contains the input tokens\n",
        "y = en_sample[1:context_size+1] # contains the targets, which are the inputs shifted by 1\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:        {y}\")\n",
        "\n",
        "# this is looping in encoding method\n",
        "for i in range(1, context_size+1):\n",
        "\n",
        "     context = en_sample[:i]\n",
        "     desired  = en_sample[i]\n",
        "\n",
        "     print(context, \"------->\", desired)\n",
        "\n",
        "# this is looping in decoding method\n",
        "for i in range(1, context_size+1):\n",
        "\n",
        "     context = en_sample[:i]\n",
        "     desired  = en_sample[i]\n",
        "\n",
        "     print(tokenizer.decode(context), \"------->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2ORPIGfFQki",
        "outputId": "63335466-a217-4567-db61-6da68bc8b95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n",
            "x: [290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686]\n",
            "y:        [4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976]\n",
            "[290] -------> 4920\n",
            "[290, 4920] -------> 2241\n",
            "[290, 4920, 2241] -------> 287\n",
            "[290, 4920, 2241, 287] -------> 257\n",
            "[290, 4920, 2241, 287, 257] -------> 4489\n",
            "[290, 4920, 2241, 287, 257, 4489] -------> 64\n",
            "[290, 4920, 2241, 287, 257, 4489, 64] -------> 319\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319] -------> 262\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262] -------> 34686\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686] -------> 41976\n",
            " and ------->  established\n",
            " and established ------->  himself\n",
            " and established himself ------->  in\n",
            " and established himself in ------->  a\n",
            " and established himself in a ------->  vill\n",
            " and established himself in a vill -------> a\n",
            " and established himself in a villa ------->  on\n",
            " and established himself in a villa on ------->  the\n",
            " and established himself in a villa on the ------->  Riv\n",
            " and established himself in a villa on the Riv -------> iera\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 2.5 A dataset for batched inputs and targets\n",
        "import urllib.request\n",
        "import re\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "\"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt) # tokenize the entire text\n",
        "\n",
        "    # Uses a sliding window to chunk the book into overlapping sequences of max_length\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "\n",
        "      input_chunk = token_ids[ i : i + max_length ]\n",
        "      target_chunk = token_ids[ i + 1 : i + max_length + 1 ]\n",
        "\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "\n",
        "  # Returns the total number of rows in the dataset\n",
        "  def __len__(self):\n",
        "\n",
        "     return len(self.input_ids)\n",
        "\n",
        "  # Returns a single row from the dataset\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# Listing 2.6 A data loader to generate batches with input-with pairs\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "   tokenizer = tiktoken.get_encoding(\"gpt2\") # Initialize the tokenizer\n",
        "\n",
        "   dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # create dataset\n",
        "   dataloader = DataLoader(\n",
        "       dataset,\n",
        "       batch_size= batch_size,\n",
        "       shuffle= shuffle,\n",
        "       drop_last= drop_last, # drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
        "       num_workers= num_workers # The number of CPU processes to use for preprocessing\n",
        "       )\n",
        "\n",
        "   return dataloader\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "     raw_text = f.read()\n",
        "\n",
        "\"\"\" The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach \"\"\"\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=8, stride=2,  shuffle=False)\n",
        "\n",
        "\n",
        "print(\"-\" * 45)\n",
        "# Converts dataloader into a Python iterator to fetch the next entry via Python’s built-in next() function\n",
        "data_itr = iter(dataloader)\n",
        "first_batch = next(data_itr)\n",
        "print(first_batch, end=\"\\n\")\n",
        "secod_batch = next(data_itr)\n",
        "print(secod_batch)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "#create another initiate from dataloader with batch_size=8, and stride=4\n",
        "dataloader = create_dataloader_v1( raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "\"\"\" Let’s see how the token ID to embedding vector conversion works with a hands-on\n",
        "example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:\"\"\"\n",
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(128)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "print(embedding_layer.weight)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "print(embedding_layer(torch.tensor([3])))\n",
        "\n",
        "print(embedding_layer(input_ids))\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embeddeing_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "max_length = 4\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"Token IDs: \\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
        "\n",
        "\"\"\" using the embedding layer to embed these token IDs into 256-dimensional vectors:\n",
        "\n",
        "    For a GPT model’s absolute embedding approach, we just need to create another embedding layer\n",
        "    that has the same embedding dimension as the token_embedding_layer: \"\"\"\n",
        "\n",
        "token_embeddeing = token_embeddeing_layer(inputs)\n",
        "\n",
        "print(token_embeddeing.shape)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "context_length  = max_length\n",
        "\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
        "\n",
        "print(pos_embedding.shape)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\"\"\" We can now add these directly to the token embeddings, where PyTorch will add\n",
        "the 4 × 256–dimensional pos_embeddings tensor to each 4 × 256–dimensional token\n",
        "embedding tensor in each of the eight batches: \"\"\"\n",
        "\n",
        "\n",
        "input_tokens = token_embeddeing + pos_embedding\n",
        "print(input_tokens.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm1AjYfsRGO_",
        "outputId": "cf96c045-cbd5-4ce3-82a7-97c022d3dc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n",
            "[tensor([[  40,  367, 2885, 1464, 1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899]])]\n",
            "[tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n",
            "---------------------------------------------\n",
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n",
            "---------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[ 1.0117, -0.2076, -0.6215],\n",
            "        [-1.2875, -0.1075,  0.0563],\n",
            "        [-0.3184,  3.1566,  0.4621],\n",
            "        [-1.6456,  1.1484, -1.5457],\n",
            "        [-0.5344,  0.1237,  0.2927],\n",
            "        [ 0.6586,  0.3008,  0.1725]], requires_grad=True)\n",
            "---------------------------------------------\n",
            "tensor([[-1.6456,  1.1484, -1.5457]], grad_fn=<EmbeddingBackward0>)\n",
            "tensor([[-0.3184,  3.1566,  0.4621],\n",
            "        [-1.6456,  1.1484, -1.5457],\n",
            "        [ 0.6586,  0.3008,  0.1725],\n",
            "        [-1.2875, -0.1075,  0.0563]], grad_fn=<EmbeddingBackward0>)\n",
            "---------------------------------------------\n",
            "Token IDs: \n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n",
            "torch.Size([8, 4, 256])\n",
            "---------------------------------------------\n",
            "torch.Size([4, 256])\n",
            "---------------------------------------------\n",
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chapter3 Craeting Context vector for evry element in Input sentances as below\n",
        "import torch\n",
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")\n",
        "\n",
        "\"\"\" to calculate attention context vector Z we will going by theree steps as below\n",
        "\n",
        "     1- Compute attention Scores W\n",
        "     2- Compute attention weights\n",
        "     3- Compute context vector Z\n",
        "     \"\"\"\n",
        "\n",
        "# 1- Compute attention Scores W\n",
        "\n",
        "query = inputs[1]\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0]) # calculating attention scores by using dot products funcation |a||b|cos0\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "\n",
        "  attn_scores_2[i] = torch.dot( x_i , query )\n",
        "\n",
        "print(attn_scores_2)\n",
        "\n",
        "# 2- Compute attention weights\n",
        "\n",
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum() # by using normalization fun to compute atten weights\n",
        "\n",
        "print(\"Attention Weights: \", attn_weights_2_tmp)\n",
        "\n",
        "print(\"Sum: \", attn_weights_2_tmp.sum())\n",
        "\n",
        "\"\"\" But In practice, it’s more common and advisable to use the softmax function for normalization\n",
        "\n",
        "    The softmax function, also known as softargmax or normalized exponential function,\n",
        "\n",
        "    converts a vector of K real numbers into a probability distribution of K possible outcomes\"\"\"\n",
        "\n",
        "def softmax_naive()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RDnNe07ejteh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b9e4c8-e29a-4813-edce-131007eb0886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
            "Attention Weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum:  tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.4.1 Computing the attention weights step by step\n",
        "\n",
        "\"\"\"\n",
        "We will implement the self-attention mechanism step by step by introducing the\n",
        "three trainable weight matrices Wq, Wk, and Wv. These three matrices are used to\n",
        "project the embedded input tokens, x(i), into query, key, and value vectors, respectively \"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")\n",
        "\n",
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "\n",
        "d_out = 2\n",
        "\n",
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "\n",
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)\n",
        "\n",
        "# to obtain all keys and values via matrix multiplication\n",
        "\n",
        "query = inputs @ W_query\n",
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(\"query.shape:\", query.shape)\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)\n",
        "\n",
        "#let’s compute the attention score ω22:\n",
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)\n",
        "\n",
        "#we can generalize this computation to all attention scores via matrix multiplication\n",
        "\n",
        "attn_scores_2 = query_2 @ keys.T\n",
        "print(\"attn_scores_2\",attn_scores_2)\n",
        "\n",
        "# We compute the attention weights by scaling the attention scores and using the softmax function\n",
        "\n",
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(\"attn_weights_2\",attn_weights_2)\n",
        "\n",
        "# compute the context vector by combining all value vectors via the attention weights.\n",
        "\n",
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(\"context_vec_2\",context_vec_2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QxtdHsfvzC_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ea0cee-3de1-4511-ae92-d6f7429d16d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n",
            "query.shape: torch.Size([6, 2])\n",
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n",
            "tensor(1.8524)\n",
            "attn_scores_2 tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
            "attn_weights_2 tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "context_vec_2 tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 3.1 A compact self-attention class\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "            keys = x @ self.W_key\n",
        "            queries = x @ self.W_query\n",
        "            values = x @ self.W_value\n",
        "            attn_scores = queries @ keys.T # omega\n",
        "            attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1 )\n",
        "            context_vec = attn_weights @ values\n",
        "            return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8ItnaLon9k5",
        "outputId": "81e1b048-7b08-4fdd-af44-75b3d972fa66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We can improve the implementation further by utilizing SelfAttention_v1 PyTorch’s\n",
        "layers, which effectively perform matrix multiplication when nn.Linear the bias units are disabled.\n",
        " Additionally, a significant advantage of using nn.Linear \"\"\"\n",
        "# Listing 3.2 A self-attention class using PyTorch’s Linear layers\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "      def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "\n",
        "            super().__init__()\n",
        "            self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "            self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "            self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "      def forward(self, x):\n",
        "\n",
        "            keys = self.W_key(x)\n",
        "            queries = self.W_query(x)\n",
        "            values = self.W_value(x)\n",
        "            attn_scores = queries @ keys.T\n",
        "            attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "            )\n",
        "            context_vec = attn_weights @ values\n",
        "            return context_vec\n",
        "\n",
        "# You can use the SelfAttention_v2 similar to SelfAttention_v1:\n",
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVHR5xHhqw5f",
        "outputId": "45010521-9b90-4a49-9253-23d95fc87b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 3.4 A wrapper class to implement multi-head attention\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "      def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "\n",
        "          super().__init__()\n",
        "          self.d_out = d_out\n",
        "          self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "          self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "          self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "          self.register_buffer(\n",
        "          'mask',\n",
        "          torch.triu(torch.ones(context_length, context_length),\n",
        "          diagonal=1)\n",
        "          )\n",
        "\n",
        "      def forward(self, x):\n",
        "          b, num_tokens, d_in = x.shape\n",
        "          keys = self.W_key(x)\n",
        "          queries = self.W_query(x)\n",
        "          values = self.W_value(x)\n",
        "          attn_scores = queries @ keys.transpose(1, 2)\n",
        "          attn_scores.masked_fill_(\n",
        "          self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "          attn_weights = torch.softmax(\n",
        "          attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "          attn_weights = self.dropout(attn_weights)\n",
        "          context_vec = attn_weights @ values\n",
        "          return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch = torch.randn(2, 4, 3)\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3, 2\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "       def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "\n",
        "            super().__init__()\n",
        "            self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "\n",
        "                 for _ in range(num_heads)])\n",
        "\n",
        "       def forward(self, x):\n",
        "\n",
        "                return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.1, 2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFOdkpeDifgC",
        "outputId": "1828cb36-d22c-4485-f1a8-1240fe1c9417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 4, 2])\n",
            "------------------------------------------------------------\n",
            "tensor([[[-4.2997e-02, -8.3028e-02,  4.5441e-02,  2.0001e-02],\n",
            "         [-6.8821e-03,  4.9498e-02,  1.0061e-02, -1.0784e-03],\n",
            "         [ 9.8406e-02,  8.0856e-02, -9.0154e-02, -8.4863e-02],\n",
            "         [ 2.1659e-01, -3.7558e-01, -5.0653e-02,  3.7103e-02]],\n",
            "\n",
            "        [[-1.2866e+00,  4.2165e-01,  1.3673e+00,  2.9141e-01],\n",
            "         [ 1.1901e+00,  4.3084e-02,  1.7666e-01, -1.2782e-01],\n",
            "         [-1.7002e-01,  1.8293e-01, -5.5676e-01, -2.3773e-01],\n",
            "         [ 1.0950e-01, -2.4061e-01,  1.4731e-01,  2.3350e-01]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.6.2 Implementing multi-head attention with weight splits\n",
        "\n",
        "import torch\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "     def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "        \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length),\n",
        "        diagonal=1)\n",
        "        )\n",
        "\n",
        "     def forward(self, x):\n",
        "\n",
        "            b, num_tokens, d_in = x.shape\n",
        "            keys = self.W_key(x)\n",
        "            queries = self.W_query(x)\n",
        "            values = self.W_value(x)\n",
        "\n",
        "            keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "            values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "            queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "            \"\"\" We implicitly split the matrix by adding a num_heads dimension. Then we unroll the last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\"\"\"\n",
        "            keys = keys.transpose(1, 2)\n",
        "            queries = queries.transpose(1, 2) # Transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads,num_tokens, head_dim)\n",
        "            values = values.transpose(1, 2)\n",
        "\n",
        "            attn_scores = queries @ keys.transpose(2, 3)\n",
        "            mask_bool = self.mask.bool()[:num_tokens, :num_tokens]  # Masks truncated to the number of tokens\n",
        "            attn_scores.masked_fill_(mask_bool, -torch.inf)  # Uses the mask to fill attention scores\n",
        "            attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "\n",
        "            context_vec = (attn_weights @ values).transpose(1, 2) # Tensor shape:(b, num_tokens,n_heads,head_dim)\n",
        "            context_vec = context_vec.contiguous().view( b, num_tokens, self.d_out ) # Combines heads, where self.d_out= self.num_heads * self.head_dim\n",
        "            context_vec = self.out_proj(context_vec) # Adds an optional linear projection\n",
        "            return context_vec\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "batch = torch.randn(2, 4, 2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "reFwW5oSpNgH",
        "outputId": "fc6e6c04-27ef-41fb-aeb1-7827fc85363b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (8x2 and 3x2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a49d7b5d1275>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mcontext_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"context_vecs.shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a49d7b5d1275>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x2 and 3x2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Implementing a GPT model from scratch to generate text \"\"\"\n",
        "\n",
        "# We specify the configuration of the small GPT-2 model via the following Python dictionary, which we will use in the code examples later\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "\n",
        "    \"vocab_size\": 50257, # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768, # Embedding dimension\n",
        "    \"n_heads\": 12, # Number of attention heads\n",
        "    \"n_layers\": 12, # Number of layers\n",
        "    \"drop_rate\": 0.1, # Dropout rate\n",
        "    \"qkv_bias\": False # Query-Key-Value bias\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# Listing 4.1 A placeholder GPT model architecture class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "\n",
        "      def __init__(self, cfg):\n",
        "\n",
        "            super().__init__()\n",
        "            self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "            self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "            self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "            self.trf_blocks = nn.Sequential(\n",
        "            *[DummyTransformerBlock(cfg)\n",
        "            for _ in range(cfg[\"n_layers\"])])\n",
        "            self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "            self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "      def forward(self, in_idx):\n",
        "\n",
        "          batch_size, seq_len = in_idx.shape\n",
        "          tok_embeds = self.tok_emb(in_idx)\n",
        "          pos_embeds = self.pos_emb(\n",
        "          torch.arange(seq_len, device=in_idx.device))\n",
        "          x = tok_embeds + pos_embeds\n",
        "          x = self.drop_emb(x)\n",
        "          x = self.trf_blocks(x)\n",
        "          x = self.final_norm(x)\n",
        "          logits = self.out_head(x)\n",
        "          return logits"
      ],
      "metadata": {
        "id": "sxDv4ldQmO9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 4.5 A neural network to illustrate shortcut connections\n",
        "\n",
        "class ExampleDeepNueralNetwork(nn.Module):\n",
        "\n",
        "      def __init__ (self, layer_sizes, use_shortcut):\n",
        "\n",
        "           super().__init__()\n",
        "           self.use_shortcut = use_shortcut\n",
        "           self.layers = nn.ModuleList([\n",
        "               nn.sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
        "               GELU()),\n",
        "              nn.sequential(nn.Linear(layer_sizes[1], layer_sizes[2])),\n",
        "               GELU()),\n",
        "               nn.sequential(nn.Linear(layer_sizes[2], layer_sizes[3])),\n",
        "               GELU()),\n",
        "               nn.sequential(nn.Linear(layer_sizes[3], layer_sizes[4])),\n",
        "               GELU(),\n",
        "               nn.sequential(nn.Linear(layer_sizes[4], layer_sizes[5])),\n",
        "               GELU()\n",
        "           ])\n",
        "\n",
        "     def forward(self, x):\n",
        "\n",
        "         for layer in self.layer:\n",
        "\n",
        "             layer_out = layer(x)\n",
        "\n",
        "             if self.use_shortcut and x.shape == layer_out.shape: # useing shortcut funcation to added input of L1 to output L1 and so on\n",
        "                 x = x + layer_out\n",
        "             else:\n",
        "                 x = layer_out\n",
        "         return x"
      ],
      "metadata": {
        "id": "k3nqbQvJyj17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))\n",
        "\n",
        "all_tokens = sorted(list(set(preprocced)))\n",
        "all_tokens.extend(\"<|endoftext|>\",\"<|unk|>\")\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "print(len(vocab.items()))\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        " print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "MK5u0yrvNpHZ",
        "outputId": "d94ac574-c3b4-4337-8186-1d28d5b0c405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-253f11091681>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<|endoftext|>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"<|unk|>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-2d70dce9494d>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     15\u001b[0m       preprocessed = [\n\u001b[1;32m     16\u001b[0m                 item.strip() for item in preprocessed if item.strip()]\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_into_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocced\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-2d70dce9494d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m       preprocessed = [\n\u001b[1;32m     16\u001b[0m                 item.strip() for item in preprocessed if item.strip()]\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_into_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocced\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_Y1g0ydGDhy",
        "outputId": "5f782811-6efb-4acb-9729-a16631acf6ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-JEqhnmyKgPq"
      }
    }
  ]
}