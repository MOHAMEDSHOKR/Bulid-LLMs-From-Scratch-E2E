
# This chapter covers
 High-level explanations of the fundamental
concepts behind large language models (LLMs)
 Insights into the transformer architecture from
which LLMs are derived
 A plan for building an LLM from scratch


Understanding large language models

so, What is LLMs ?

Large language models (LLMs), such as those offered in OpenAI’s ChatGPT, are
deep neural network models that have been developed over the past few years.
They ushered in a new era for natural language processing (NLP).


1.1 What is an LLM?
An LLM is a neural network designed to understand, generate, and respond to humanlike
text. These models are deep neural networks trained on massive amounts of text
data, sometimes encompassing large portions of the entire publicly available text on
the internet.
The “large” in “large language model” refers to both the model’s size in terms of
parameters and the immense dataset on which it’s trained. Models like this often have
tens or even hundreds of billions of parameters, which are the adjustable weights in
the network that are optimized during training to predict the next word in a sequence.
Next-word prediction is sensible because it harnesses the inherent sequential nature
of language to train models on understanding context, structure, and relationships
within text. Yet, it is a very simple task, and so it is surprising to many researchers that
it can produce such capable models. In later chapters, we will discuss and implement
the next-word training procedure step by step.

LLMs utilize an architecture called the transformer, which allows them to pay selective
attention to different parts of the input when making predictions, making them
especially adept at handling the nuances and complexities of human language.
Since LLMs are capable of generating text, LLMs are also often referred to as a form
of generative artificial intelligence, often abbreviated as generative AI or GenAI. As illustrated
in figure 1.1, AI encompasses the broader field of creating machines that can
perform tasks requiring human-like intelligence, including understanding language,
recognizing patterns, and making decisions, and includes subfields like machine
learning and deep learning.


# 1.2 Applications of LLMs
Owing to their advanced capabilities to parse and understand unstructured text data,
LLMs have a broad range of applications across various domains. Today, LLMs are
employed for machine translation, generation of novel texts (see figure 1.2), sentiment
analysis, text summarization, and many other tasks. LLMs have recently been
used for content creation, such as writing fiction, articles, and even computer code.
LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s
ChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries
and augment traditional search engines such as Google Search or Microsoft Bing.
Moreover, LLMs may be used for effective knowledge retrieval from vast volumes
of text in specialized areas such as medicine or law. This includes sifting through documents,
summarizing lengthy passages, and answering technical questions.
In short, LLMs are invaluable for automating almost any task that involves parsing
and generating text. Their applications are virtually endless, and as we continue to
innovate and explore new ways to use these models, it’s clear that LLMs have the
potential to redefine our relationship with technology, making it more conversational,
intuitive, and accessible.


We will focus on understanding how LLMs work from the ground up, coding an
LLM that can generate texts. You will also learn about techniques that allow LLMs to
carry out queries, ranging from answering questions to summarizing text, translating
text into different languages, and more. In other words, you will learn how complex
LLM assistants such as ChatGPT work by building one step by step.








