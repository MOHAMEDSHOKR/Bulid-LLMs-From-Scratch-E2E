
# This chapter covers
 High-level explanations of the fundamental
concepts behind large language models (LLMs)
 Insights into the transformer architecture from
which LLMs are derived
 A plan for building an LLM from scratch


Understanding large language models

so, What is LLMs ?

Large language models (LLMs), such as those offered in OpenAI’s ChatGPT, are
deep neural network models that have been developed over the past few years.
They ushered in a new era for natural language processing (NLP).


1.1 What is an LLM?
An LLM is a neural network designed to understand, generate, and respond to humanlike
text. These models are deep neural networks trained on massive amounts of text
data, sometimes encompassing large portions of the entire publicly available text on
the internet.
The “large” in “large language model” refers to both the model’s size in terms of
parameters and the immense dataset on which it’s trained. Models like this often have
tens or even hundreds of billions of parameters, which are the adjustable weights in
the network that are optimized during training to predict the next word in a sequence.
Next-word prediction is sensible because it harnesses the inherent sequential nature
of language to train models on understanding context, structure, and relationships
within text. Yet, it is a very simple task, and so it is surprising to many researchers that
it can produce such capable models. In later chapters, we will discuss and implement
the next-word training procedure step by step.

LLMs utilize an architecture called the transformer, which allows them to pay selective
attention to different parts of the input when making predictions, making them
especially adept at handling the nuances and complexities of human language.
Since LLMs are capable of generating text, LLMs are also often referred to as a form
of generative artificial intelligence, often abbreviated as generative AI or GenAI. As illustrated
in figure 1.1, AI encompasses the broader field of creating machines that can
perform tasks requiring human-like intelligence, including understanding language,
recognizing patterns, and making decisions, and includes subfields like machine
learning and deep learning.


1.2 Applications of LLMs
Owing to their advanced capabilities to parse and understand unstructured text data,
LLMs have a broad range of applications across various domains. Today, LLMs are
employed for machine translation, generation of novel texts (see figure 1.2), sentiment
analysis, text summarization, and many other tasks. LLMs have recently been
used for content creation, such as writing fiction, articles, and even computer code.
LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s
ChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries
and augment traditional search engines such as Google Search or Microsoft Bing.
Moreover, LLMs may be used for effective knowledge retrieval from vast volumes
of text in specialized areas such as medicine or law. This includes sifting through documents,
summarizing lengthy passages, and answering technical questions.
In short, LLMs are invaluable for automating almost any task that involves parsing
and generating text. Their applications are virtually endless, and as we continue to
innovate and explore new ways to use these models, it’s clear that LLMs have the
potential to redefine our relationship with technology, making it more conversational,
intuitive, and accessible.


We will focus on understanding how LLMs work from the ground up, coding an
LLM that can generate texts. You will also learn about techniques that allow LLMs to
carry out queries, ranging from answering questions to summarizing text, translating
text into different languages, and more. In other words, you will learn how complex
LLM assistants such as ChatGPT work by building one step by step.


1.3 Stages of building and using LLMs
Why should we build our own LLMs? Coding an LLM from the ground up is an excellent
exercise to understand its mechanics and limitations. Also, it equips us with the
required knowledge for pretraining or fine-tuning existing open source LLM architectures
to our own domain-specific datasets or tasks.


The first step in creating an LLM is to train it on a large corpus of text data, sometimes
referred to as raw text. Here, “raw” refers to the fact that this data is just regular text
without any labeling information. (Filtering may be applied, such as removing formatting
characters or documents in unknown languages.)


This first training stage of an LLM is also known as pretraining, creating an initial pretrained
LLM, often called a base or foundation model. A typical example of such a model
is the GPT-3 model (the precursor of the original model offered in ChatGPT). This
model is capable of text completion—that is, finishing a half-written sentence provided
by a user. It also has limited few-shot capabilities, which means it can learn to
perform new tasks based on only a few examples instead of needing extensive training
data.


After obtaining a pretrained LLM from training on large text datasets, where the
LLM is trained to predict the next word in the text, we can further train the LLM on
labeled data, also known as fine-tuning.
The two most popular categories of fine-tuning LLMs are instruction fine-tuning and
classification fine-tuning. In instruction fine-tuning, the labeled dataset consists of
instruction and answer pairs, such as a query to translate a text accompanied by the
correctly translated text. In classification fine-tuning, the labeled dataset consists of
texts and associated class labels—for example, emails associated with “spam” and “not
spam” labels.
We will cover code implementations for pretraining and fine-tuning an LLM, and
we will delve deeper into the specifics of both instruction and classification fine-tuning
after pretraining a base LLM.


1.4 Introducing the transformer architecture
Most modern LLMs rely on the transformer architecture, which is a deep neural network
architecture introduced in the 2017 paper “Attention Is All You Need” (https://
arxiv.org/abs/1706.03762). To understand LLMs, we must understand the original
transformer, which was developed for machine translation, translating English texts to
German and French. A simplified version of the transformer architecture is depicted
in figure 1.4.
The transformer architecture consists of two submodules: an encoder and a
decoder. The encoder module processes the input text and encodes it into a series of
numerical representations or vectors that capture the contextual information of the
input. Then, the decoder module takes these encoded vectors and generates the output
text. In a translation task, for example, the encoder would encode the text from
the source language into vectors, and the decoder would decode these vectors to generate
text in the target language. Both the encoder and decoder consist of many layers
connected by a so-called self-attention mechanism. You may have many questions
regarding how the inputs are preprocessed and encoded. These will be addressed in a
step-by-step implementation in subsequent chapters.
A key component of transformers and LLMs is the self-attention mechanism (not
shown), which allows the model to weigh the importance of different words or tokens
in a sequence relative to each other. This mechanism enables the model to capture
long-range dependencies and contextual relationships within the input data, enhancing
its ability to generate coherent and contextually relevant output. However, due to
its complexity, we will defer further explanation to chapter 3, where we will discuss
and implement it step by step.

Later variants of the transformer architecture, such as BERT (short for bidirectional
encoder representations from transformers) and the various GPT models (short for generative
pretrained transformers), built on this concept to adapt this architecture for different
tasks. If interested, refer to appendix B for further reading suggestions.
BERT, which is built upon the original transformer’s encoder submodule, differs
in its training approach from GPT. While GPT is designed for generative tasks, BERT
and its variants specialize in masked word prediction, where the model predicts masked

or hidden words in a given sentence, as shown in figure 1.5. This unique training strategy
equips BERT with strengths in text classification tasks, including sentiment prediction
and document categorization. As an application of its capabilities, as of this writing, X
(formerly Twitter) uses BERT to detect toxic content.

GPT, on the other hand, focuses on the decoder portion of the original transformer
architecture and is designed for tasks that require generating texts. This includes
machine translation, text summarization, fiction writing, writing computer code,
and more.
GPT models, primarily designed and trained to perform text completion tasks,
also show remarkable versatility in their capabilities. These models are adept at executing
both zero-shot and few-shot learning tasks. Zero-shot learning refers to the ability
to generalize to completely unseen tasks without any prior specific examples. On
the other hand, few-shot learning involves learning from a minimal number of examples
the user provides as input




