## LLMs, such as GPT (which stands for generative pretrained transformer), are large deep neural network architectures designed to generate new text one word (or token) at a time.

However, despite their size, the model architecture is less complicated than you
might think, since many of its components are repeated, as we will see later.

Figure 4.2 provides a top-down view of a GPT-like LLM, with its main components highlighted.
We have already covered several aspects of the LLM architecture, such as input
tokenization and embedding and the masked multi-head attention module. 

Now, we will implement the core structure of the GPT model, including its transformer blocks,
which we will later train to generate human-like text.

Previously, we used smaller embedding dimensions for simplicity, ensuring that the
concepts and examples could comfortably fit on a single page.

Now, we are scaling up to the size of a small GPT-2 model, specifically the smallest version with 124 million
parameters, as described in “Language Models Are Unsupervised Multitask Learners,”
by Radford et al. (https://mng.bz/yoBq). 

### Note 
 { that while the original report mentions 117 million parameters, this was later corrected.
   In chapter 6, we will focus on loading pretrained weights into our implementation and adapting it for larger GPT-2
    models with 345, 762, and 1,542 million parameters.
    In the context of deep learning and LLMs like GPT, the term “parameters” refers
    to the trainable weights of the model. These weights are essentially the internal variables
    of the model that are adjusted and optimized during the training process to
    minimize a specific loss function. This optimization allows the model to learn from
    the training data }.

-------------------------------------------------------------------------------------------------



Figure 4.2 A GPT model. In addition to the embedding layers, it consists of one or more
transformer blocks containing the masked multi-head attention module we previously
implemented.

-------------------------------------------------------------------------------------------------

For example, in a neural network layer that is represented by a 2,048 × 2,048–dimensional
matrix (or tensor) of weights, each element of this matrix is a parameter. Since there
are 2,048 rows and 2,048 columns, the total number of parameters in this layer is 2,048
multiplied by 2,048, which equals 4,194,304 parameters.


## GPT-2 vs. GPT-3
Note that we are focusing on GPT-2 because OpenAI has made the weights of the
pretrained model publicly available, which we will load into our implementation in
chapter 6. GPT-3 is fundamentally the same in terms of model architecture, except
that it is scaled up from 1.5 billion parameters in GPT-2 to 175 billion parameters
in GPT-3, and it is trained on more data. As of this writing, the weights for GPT-3
are not publicly available. GPT-2 is also a better choice for learning how to implement
LLMs, as it can be run on a single laptop computer, whereas GPT-3 requires a
GPU cluster for training and inference. According to Lambda Labs (https://lambdalabs
.com/), it would take 355 years to train GPT-3 on a single V100 datacenter GPU
and 665 years on a consumer RTX 8000 GPU.


We specify the configuration of the small GPT-2 model via the following Python dictionary,
which we will use in the code examples later:

    GPT_CONFIG_124M = {
             "vocab_size": 50257, # Vocabulary size
             "context_length": 1024, # Context length
             "emb_dim": 768, # Embedding dimension
             "n_heads": 12, # Number of attention heads
             "n_layers": 12, # Number of layers
             "drop_rate": 0.1, # Dropout rate
             "qkv_bias": False # Query-Key-Value bias
    }

In the GPT_CONFIG_124M dictionary, we use concise variable names for clarity and to
prevent long lines of code:

* vocab_size refers to a vocabulary of 50,257 words, as used by the BPE tokenizer(see chapter 2).
  
* context_length denotes the maximum number of input tokens the model can handle via the positional embeddings (see chapter 2).

* emb_dim represents the embedding size, transforming each token into a 768-dimensional vector.
  
* n_heads indicates the count of attention heads in the multi-head attention mechanism (see chapter 3).
  
* n_layers specifies the number of transformer blocks in the model, which we will cover in the upcoming discussion.
  
* drop_rate indicates the intensity of the dropout mechanism (0.1 implies a 10% random drop out of hidden units) to prevent overfitting (see chapter 3).
  
* qkv_bias determines whether to include a bias vector in the Linear layers of
the multi-head attention for query, key, and value computations. We will initially
disable this, following the norms of modern LLMs, but we will revisit it in chapter
6 when we load pretrained GPT-2 weights from OpenAI into our model (see chapter 6).



Using this configuration, we will implement a GPT placeholder architecture (Dummy-
GPTModel), as shown in figure 4.3. This will provide us with a big-picture view of how
everything fits together and what other components we need to code to assemble the
full GPT model architecture.

The numbered boxes in figure 4.3 illustrate the order in which we tackle the individual
concepts required to code the final GPT architecture. We will start with step 1,
a placeholder GPT backbone we will call DummyGPTModel.

