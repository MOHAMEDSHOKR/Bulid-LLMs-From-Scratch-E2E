# Compact causal attention class

We will now incorporate the causal attention and dropout modifications into the
SelfAttention Python class we developed in section 3.4. 

This class will then serve as a template for developing multi-head attention, which is the final attention class we will implement.

But before we begin, let’s ensure that the code can handle batches consisting of
more than one input so that the CausalAttention class supports the batch outputs
produced by the data loader we implemented in chapter 2.

For simplicity, to simulate such batch inputs, we duplicate the input text example:

    batch = torch.stack((inputs, inputs), dim=0)
    print(batch.shape)

This results in a three-dimensional tensor consisting of two input texts with six tokens
each, where each token is a three-dimensional embedding vector:
    
    torch.Size([2, 6, 3])

The following CausalAttention class is similar to the SelfAttention class we implemented
earlier, except that we added the dropout and causal mask components.

## Listing 3.3 A compact causal attention class

    class CausalAttention(nn.Module):

              def __init__(self, d_in, d_out, context_length,
                      dropout, qkv_bias=False):
                      
                      super().__init__()
                      self.d_out = d_out
                      self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
                      self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
                      self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
                      self.dropout = nn.Dropout(dropout)  --------> Compared to the previous SelfAttention_v1 class, we added a dropout layer.
                      self.register_buffer(
                      'mask',
                      torch.triu(torch.ones(context_length, context_length),
                      diagonal=1) ------> The register_buffer call is also a new addition (more information is provided in the following text).
                      )
                      
              def forward(self, x):
              
                      b, num_tokens, d_in = x.shape
                      keys = self.W_key(x)
                      queries = self.W_query(x)
                      values = self.W_value(x)
                      attn_scores = queries @ keys.transpose(1, 2) ----> We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).
                      attn_scores.masked_fill_(
                      self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) --> In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory copies. 
                      attn_weights = torch.softmax(
                      attn_scores / keys.shape[-1]**0.5, dim=-1)
                      attn_weights = self.dropout(attn_weights)
                      context_vec = attn_weights @ values
                      return context_vec


While all added code lines should be familiar at this point, we now added a self
.register_buffer() call in the __init__ method.

The use of register_buffer in PyTorch is not strictly necessary for all use cases but offers several advantages here. 
For instance, when we use the CausalAttention class in our LLM, buffers are automatically
moved to the appropriate device (CPU or GPU) along with our model, which will
be relevant when training our LLM. 


This means we don’t need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors.
We can use the CausalAttention class as follows, similar to SelfAttention previously:


    torch.manual_seed(123)
    context_length = batch.shape[1]
    ca = CausalAttention(d_in, d_out, context_length, 0.0)
    context_vecs = ca(batch)
    print("context_vecs.shape:", context_vecs.shape)


The resulting context vector is a three-dimensional tensor where each token is now
represented by a two-dimensional embedding:


    context_vecs.shape: torch.Size([2, 6, 2])

Figure 3.23 summarizes what we have accomplished so far. We have focused on the
concept and implementation of causal attention in neural networks. Next, we will
expand on this concept and implement a multi-head attention module that implements
several causal attention mechanisms in parallel.


![image](https://github.com/user-attachments/assets/6eaeae1f-eba5-4bb7-a4d5-efced28f6585)
Figure 3.23 Here’s what we’ve done so far. We began with a simplified attention mechanism, added trainable
weights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code
multi-head attention, which we will use in our LLM.
