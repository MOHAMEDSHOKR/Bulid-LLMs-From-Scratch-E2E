# Causal Attention
Causal attention, also known as masked self-attention, is a type of attention mechanism used in autoregressive models (e.g., GPT, decoder-only Transformers) 
to ensure that predictions for a token at position i depend only on previous tokens (positions 1 to i−1) and the current token itself. 
It enforces a causal relationship by preventing the model from "peeking" at future tokens during training or inference.

### Key Concepts

Purpose:

Maintain the autoregressive property: Generate sequences one token at a time, where each new token depends only on previously generated tokens.

Prevent information leakage from future tokens, which could artificially inflate model performance during training.

Implementation:

* A mask is applied to the attention scores before the softmax operation. This mask ensures that for a token at position i, all positions j >i (future tokens) are ignored.

* The mask is typically a lower-triangular matrix (values set to 0 for j ≤ i and −∞ for j>i):

![image](https://github.com/user-attachments/assets/6aced04a-cf3e-4e4e-9fd8-a4f6e2ae8e21)

* After applying the mask, softmax assigns zero probability to future positions, effectively removing their influence.

For many LLM tasks, you will want the self-attention mechanism to consider only the
tokens that appear prior to the current position when predicting the next token in a
sequence. 

Causal attention, also known as masked attention, is a specialized form of selfattention.
It restricts a model to only consider previous and current inputs in a sequence
when processing any given token when computing attention scores.

This is in contrast to the standard self-attention mechanism, which allows access to the entire input
sequence at once.

Now, we will modify the standard self-attention mechanism to create a causal
attention mechanism, which is essential for developing an LLM in the subsequent
chapters.

To achieve this in GPT-like LLMs, for each token processed, we mask out
the future tokens, which come after the current token in the input text, as illustrated
in figure 3.19. We mask out the attention weights above the diagonal, and we normalize the nonmasked attention weights such that the attention weights sum to 1 in
each row. Later, we will implement this masking and normalization procedure in code.

![image](https://github.com/user-attachments/assets/13026b80-1e14-4ed4-9639-929862db8029)
Figure 3.19 In causal attention, we mask out the attention weights above the diagonal such that for
a given input, the LLM can’t access future tokens when computing the context vectors using the
attention weights. For example, for the word “journey” in the second row, we only keep the attention
weights for the words before (“Your”) and in the current position (“journey”).


-----------------------------------------------------------------------------------------------

## 3.5.1 Applying a causal attention mask

Our next step is to implement the causal attention mask in code. To implement the
steps to apply a causal attention mask to obtain the masked attention weights, as summarized
in figure 3.20, let’s work with the attention scores and weights from the previous
section to code the causal attention mechanism.

![image](https://github.com/user-attachments/assets/a8a1f563-53d8-4a32-ae41-3094c018fa91)
Figure 3.20 One way to obtain the masked attention weight matrix in causal attention is to apply the
softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing
the resulting matrix.

* In the first step, we compute the attention weights using the softmax function as we
have done previously:
  
      queries = sa_v2.W_query(inputs) -------------------> Reuses the query and key weight matrices of the SelfAttention_v2 object from the previous section for convenience
      keys = sa_v2.W_key(inputs)
      attn_scores = queries @ keys.T
      attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
      print(attn_weights)
    
* This results in the following attention weights:

      tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],
              [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],
              [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],
              [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],
              [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],
              [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
              grad_fn=<SoftmaxBackward0>)
    
* We can implement the second step using PyTorch’s tril function to create a mask
where the values above the diagonal are zero:

      context_length = attn_scores.shape[0]
      mask_simple = torch.tril(torch.ones(context_length, context_length))
      print(mask_simple)

  The resulting mask is

      tensor([[1., 0., 0., 0., 0., 0.],
              [1., 1., 0., 0., 0., 0.],
              [1., 1., 1., 0., 0., 0.],
              [1., 1., 1., 1., 0., 0.],
              [1., 1., 1., 1., 1., 0.],
              [1., 1., 1., 1., 1., 1.]])

   Now, we can multiply this mask with the attention weights to zero-out the values above
the diagonal:

      masked_simple = attn_weights*mask_simple
      print(masked_simple)
    
  As we can see, the elements above the diagonal are successfully zeroed out:

        tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
                [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],
                [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],
                [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],
                [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],
                [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
                grad_fn=<MulBackward0>)
    
* The third step is to renormalize the attention weights to sum up to 1 again in each
row. We can achieve this by dividing each element in each row by the sum in each row:

      row_sums = masked_simple.sum(dim=-1, keepdim=True)
      masked_simple_norm = masked_simple / row_sums
      print(masked_simple_norm)
  
The result is an attention weight matrix where the attention weights above the diagonal
are zeroed-out, and the rows sum to 1:

    tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],
            [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],
            [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],
            [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
            grad_fn=<DivBackward0>)

## Information leakage

When we apply a mask and then renormalize the attention weights, it might initially
appear that information from future tokens (which we intend to mask) could still influence
the current token because their values are part of the softmax calculation. However,
the key insight is that when we renormalize the attention weights after masking,
what we’re essentially doing is recalculating the softmax over a smaller subset (since
masked positions don’t contribute to the softmax value).

The mathematical elegance of softmax is that despite initially including all positions
in the denominator, after masking and renormalizing, the effect of the masked positions
is nullified—they don’t contribute to the softmax score in any meaningful way.
In simpler terms, after masking and renormalization, the distribution of attention
weights is as if it was calculated only among the unmasked positions to begin with.
This ensures there’s no information leakage from future (or otherwise masked)
tokens as we intended.


While we could wrap up our implementation of causal attention at this point, we can
still improve it. Let’s take a mathematical property of the softmax function and implement
the computation of the masked attention weights more efficiently in fewer steps,
as shown in figure 3.21.

![image](https://github.com/user-attachments/assets/6885b749-1758-48a3-affa-0d115e8a9a9f)
Figure 3.21 A more efficient way to obtain the masked attention weight matrix in
causal attention is to mask the attention scores with negative infinity values before
applying the softmax function.


The softmax function converts its inputs into a probability distribution. When negative
infinity values (-∞) are present in a row, the softmax function treats them as zero
probability. (Mathematically, this is because e –∞ approaches 0.)
We can implement this more efficient masking “trick” by creating a mask with 1s
above the diagonal and then replacing these 1s with negative infinity (-inf) values:

    
    mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
    masked = attn_scores.masked_fill(mask.bool(), -torch.inf)
    print(masked)
This results in the following mask:

    tensor([[0.2899, -inf, -inf, -inf, -inf, -inf],
    [0.4656, 0.1723, -inf, -inf, -inf, -inf],
    [0.4594, 0.1703, 0.1731, -inf, -inf, -inf],
    [0.2642, 0.1024, 0.1036, 0.0186, -inf, -inf],
    [0.2183, 0.0874, 0.0882, 0.0177, 0.0786, -inf],
    [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],
    grad_fn=<MaskedFillBackward0>)


Now all we need to do is apply the softmax function to these masked results, and we
are done:
    
    attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)
    print(attn_weights)


As we can see based on the output, the values in each row sum to 1, and no further
normalization is necessary:


    tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
    [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],
    [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],
    [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],
    [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],
    [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
    grad_fn=<SoftmaxBackward0>)


We could now use the modified attention weights to compute the context vectors via

      context_vec = attn_weights @ values
      
as in section 3.4. However, we will first cover another minor tweak to the causal attention mechanism that is useful for reducing
overfitting when training LLMs.
