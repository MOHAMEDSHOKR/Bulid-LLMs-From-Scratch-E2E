# Self-attention 
We’ll now cover the inner workings of the self-attention mechanism and learn how to
code it from the ground up.

Self-attention serves as the cornerstone of every LLM based on the transformer architecture.
This topic may require a lot of focus and attention(no pun intended), but once you grasp its fundamentals, you will have conquered
one of the toughest aspects of this book and LLM implementation in general.

# The “self” in self-attention
In self-attention, the “self” refers to the mechanism’s ability to compute attention
weights by relating different positions within a single input sequence. 

It assesses and learns the relationships and dependencies between various parts of the input itself,
such as words in a sentence or pixels in an image.

This is in contrast to traditional attention mechanisms, where the focus is on the relationships
between elements of two different sequences, such as in sequence-tosequence
models where the attention might be between an input sequence and an
output sequence, such as the example depicted in figure 3.5.

Since self-attention can appear complex, especially if you are encountering it for the
first time, we will begin by examining a simplified version of it. Then we will implement
the self-attention mechanism with trainable weights used in LLMs.

# 3.3.1 A simple self-attention mechanism without trainable weights

Let’s begin by implementing a simplified variant of self-attention, free from any trainable
weights, as summarized in figure 3.7. 
The goal is to illustrate a few key concepts in self-attention before adding trainable weights.

![image](https://github.com/user-attachments/assets/eb796920-956f-4ae4-9d38-ea74c2989255)
Figure 3.7 The goal of self-attention is to compute a context vector for each input
element that combines information from all other input elements. In this example,
we compute the context vector z(2). The importance or contribution of each input
element for computing z(2) is determined by the attention weights 21 to 2T. When
computing z(2), the attention weights are calculated with respect to input element
x(2) and all other inputs.

----------------------------------------------------------------------------------------

Figure 3.7 shows an input sequence, denoted as x, consisting of T elements represented
as x(1) to x(T). 

This sequence typically represents text, such as a sentence, that has already been transformed into token embeddings.
For example, consider an input text like “Your journey starts with one step.” In this
case, each element of the sequence, such as x(1), corresponds to a d-dimensional
embedding vector representing a specific token, like “Your.” Figure 3.7 shows these
input vectors as three-dimensional embeddings.

In self-attention, our goal is to calculate [ context vectors z(i) ] for each element x(i)
in the input sequence. A context vector can be interpreted as an [ enriched embedding vector ].

To illustrate this concept, let’s focus on the embedding vector of the second input
element, x(2) (which corresponds to the token “journey”), and the corresponding context
vector, z(2), shown at the bottom of figure 3.7.

This enhanced context vector, z(2), is an embedding that contains information about x(2) and all other input elements,
x(1) to x(T).

###  [ Context vectors play a crucial role in self-attention ]. 

#### Their purpose is to create enriched representations of each element in an input sequence (like a sentence)
by incorporating information from all other elements in the sequence (figure 3.7).
This is essential in LLMs, which need to understand the relationship and relevance
of words in a sentence to each other.

Later, we will add trainable weights that help an LLM learn to construct these context vectors so that they are relevant for the
LLM to generate the next token. 

But first, let’s implement a simplified self-attention mechanism to compute these weights and the resulting context vector one
step at a time.

Consider the following input sentence, which has already been embedded into
three-dimensional vectors (see chapter 2). I’ve chosen a small embedding dimension
to ensure it fits on the page without line breaks:

    import torch
    inputs = torch.tensor(
              [[0.43, 0.15, 0.89], # Your (x^1)
              [0.55, 0.87, 0.66], # journey (x^2)
              [0.57, 0.85, 0.64], # starts (x^3)
              [0.22, 0.58, 0.33], # with (x^4)
              [0.77, 0.25, 0.10], # one (x^5)
              [0.05, 0.80, 0.55]] # step (x^6)
              )
### The first step of implementing self-attention is to compute the intermediate values ω,
referred to as attention scores, as illustrated in figure 3.8. Due to spatial constraints,
the figure displays the values of the preceding inputs tensor in a truncated version;
for example, 0.87 is truncated to 0.8. In this truncated version, the embeddings of the
words “journey” and “starts” may appear similar by random chance.

![qkv](https://github.com/user-attachments/assets/7b5ac6dc-4d48-47c5-a58b-00690085c23a)

![image](https://github.com/user-attachments/assets/d0c3335f-2d80-4317-b9d3-d31da05e6e6d)
Figure 3.8 The overall goal is to illustrate the computation of the context vector z(2) using the
second input element, x(2) as a query. This figure shows the first intermediate step, computing the
attention scores  between the query x(2) and all other input elements as a dot product. (Note that
the numbers are truncated to one digit after the decimal point to reduce visual clutter.)

------------------------------------------------------------------------------------------


Figure 3.8 illustrates how we calculate the intermediate attention scores ω between the
query token and each input token. 
We determine these scores by computing the dot product of the query, x(2), with every other input token:


      query = inputs[1]
      attn_scores_2 = torch.empty(inputs.shape[0])
      for i, x_i in enumerate(inputs):
      attn_scores_2[i] = torch.dot(x_i, query)
      print(attn_scores_2)
      
The computed attention scores are

      tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])

## Understanding dot products
A dot product is essentially a concise way of multiplying two vectors element-wise and
then summing the products, which can be demonstrated as follows:

      res = 0.
      for idx, element in enumerate(inputs[0]):
      res += inputs[0][idx] * query[idx]
      print(res)
      print(torch.dot(inputs[0], query))

The output confirms that the sum of the element-wise multiplication gives the same
results as the dot product:
      
      tensor(0.9544)
      tensor(0.9544)

## 1. What is a Dot Product?
The dot product (or scalar product) is a mathematical operation that takes two vectors of the same length, multiplies their corresponding elements, and sums the results. Mathematically, for vectors a and b:

![image](https://github.com/user-attachments/assets/c2c38fa0-fb0d-4541-92b7-7972215131ce)

![direction-cosines](https://github.com/user-attachments/assets/50ea2736-7404-4ed5-8b52-58bc1ca6f54a)

## 2. Code Example Explained

Your code computes the dot product manually using a loop and compares it to PyTorch’s torch.dot function. Let’s dissect this:

Manual Calculation (Loop):

    res = 0.
    for idx, element in enumerate(inputs[0]):
        res += inputs[0][idx] * query[idx]
    print(res)  # Output: sum(inputs[0][i] * query[i] for all i)


Goal: Iterate over each element of the two vectors (inputs[0] and query), multiply corresponding elements, and accumulate the sum.

## How it works:

  inputs[0] and query are vectors (1D tensors/arrays).
  
  For each index idx, multiply inputs[0][idx] and query[idx].
  
  Sum all these products into res.

PyTorch Equivalent:

    print(torch.dot(inputs[0], query))  # Same result as the loop!
    torch.dot() is an optimized, vectorized implementation of the dot product.

Why use it? Faster (uses hardware acceleration) and avoids manual loops.

## 3. Key Properties of Dot Products

### Measures Similarity:

  1. A large dot product indicates the vectors are "similar" in direction (aligned).
  
  2. A dot product of 0 means the vectors are orthogonal (perpendicular).
  
  3. Geometric Interpretation:
  
      a ⋅ b = ∥ a ∥ ∥ b ∥ cos (θ)

      Where θ is the angle between the vectors, and ∥a∥ is the magnitude (length) of a.

### Used Everywhere in ML:

  1. Neural networks (e.g., fully connected layers compute dot products).
      
  2. Attention mechanisms (queries and keys interact via dot products).
      
  3. Similarity metrics (e.g., cosine similarity).

** Beyond viewing the dot product operation as a mathematical tool that combines
two vectors to yield a scalar value, the dot product is a measure of similarity
because it quantifies how closely two vectors are aligned: a higher dot product indicates
a greater degree of alignment or { similarity } between the vectors.**

** In the context of self-attention mechanisms, the dot product determines the extent to which
each element in a sequence focuses on, or “attends to,” any other element: [ the
higher the dot product, the higher the similarity and attention score between two elements ].**

-------------------------------------------------------------------------------------------------

### In the next step (Will Compute Attention Weights α )
as shown in figure 3.9, we normalize each of the attention scores we computed previously. 

The main goal behind the normalization is to obtain attention weights that sum up to 1. 

This normalization is a convention that is useful for interpretation and maintaining training stability in an LLM. 

Here’s a straightforward method for achieving this normalization step:
        
        attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()
        print("Attention weights:", attn_weights_2_tmp)
        print("Sum:", attn_weights_2_tmp.sum())

![image](https://github.com/user-attachments/assets/ef360cc6-466e-457d-95fc-94a3a3fc4d5d)
Figure 3.9 After computing the attention scores 21 to 2T with respect to the input query x(2), the next
step is to obtain the attention weights 21 to 2T by normalizing the attention scores.

As the output shows, the attention weights now sum to 1:

        Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])
        Sum: tensor(1.0000)


### In practice, it’s more common and advisable to use the [ softmax function ] for normalization.
This approach is better at managing extreme values and offers more favorable gradient properties during training. 

The following is a basic implementation of the softmax function for normalizing the attention scores:
        
        def softmax_naive(x):
        return torch.exp(x) / torch.exp(x).sum(dim=0)
        attn_weights_2_naive = softmax_naive(attn_scores_2)
        print("Attention weights:", attn_weights_2_naive)
        print("Sum:", attn_weights_2_naive.sum())
        
As the output shows, the softmax function also meets the objective and normalizes the
attention weights such that they sum to 1:

        Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
        Sum: tensor(1.)

### What is a Softmax Funcation 
Softmax is a mathematical function commonly used in machine learning, particularly for multi-class classification tasks. 
It converts a vector of real numbers (logits) into a probability distribution, where each value is between 0 and 1, and the sum of all values equals 1. 
Here's a detailed explanation:

### Formula
For an input vector For an input vector z = [z1,z2,...,zn] with n classes, the Softmax output for the i-th class is:

Key Properties:

1. Probability Interpretation: Outputs are probabilities, making it ideal for classification.
    
2. Amplification of Differences: Exponentials emphasize larger values, making dominant scores more pronounced.
    
3. Numerical Stability: To avoid overflow, inputs are often shifted by their maximum value:

   ![image](https://github.com/user-attachments/assets/0767dcea-c5d3-44c6-a3cc-c068f2ae806b)

Example
For z=[2,1,0.1]

Exponentiate: 
[e 2 ,e 1 ,e 0.1 ] ≈ [7.389,2.718,1.105].

Sum: 7.389+2.718+1.105 ≈ 11.212.

Normalize: Probabilities ≈ [0.659,0.242,0.099].

In addition, the softmax function ensures that the attention weights are always positive.
This makes the output interpretable as probabilities or relative importance, where higher weights indicate greater importance.

### Note 
this naive softmax implementation (softmax_naive) may encounter
numerical instability problems, such as overflow and underflow, when dealing with
large or small input values.

Therefore, in practice, it’s advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance:

        attn_weights_2 = torch.softmax(attn_scores_2, dim=0)
        print("Attention weights:", attn_weights_2)
        print("Sum:", attn_weights_2.sum())
        
In this case, it yields the same results as our previous softmax_naive function:

        Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
        Sum: tensor(1.)


Now that we have computed the normalized attention weights, we are ready for the
final step, as shown in figure 3.10: 

### calculating the context vector z(2) 
by multiplying the embedded input tokens, x(i), with the corresponding attention weights and then summing the resulting vectors. 
Thus, context vector z(2) is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:

        query = inputs[1]  ---------------> The second input token is the query
        context_vec_2 = torch.zeros(query.shape)
        for i,x_i in enumerate(inputs):
        context_vec_2 += attn_weights_2[i]*x_i
        print(context_vec_2)
        
The results of this computation are

        tensor([0.4419, 0.6515, 0.5683])

![image](https://github.com/user-attachments/assets/e2c03dac-f1e4-4364-a566-54d6cb5b7753)
Figure 3.10 The final step, after calculating and normalizing the attention scores to obtain the
attention weights for query x(2), is to compute the context vector z(2). This context vector is a
combination of all input vectors x(1) to x(T) weighted by the attention weights.

Next, we will generalize this procedure for computing context vectors to calculate all
context vectors simultaneously.

-----------------------------------------------------------------------------------------------

## 3.3.2 Computing attention weights for all input tokens

So far, we have computed attention weights and the context vector for input 2, as
shown in the highlighted row in figure 3.11. Now let’s extend this computation to calculate
attention weights and context vectors for all inputs.

![image](https://github.com/user-attachments/assets/d081f7c7-b49e-4d86-98bf-8aab02e19def)
Figure 3.11 The highlighted row shows the attention weights for the second
input element as a query. Now we will generalize the computation to obtain
all other attention weights. (Please note that the numbers in this figure are
truncated to two digits after the decimal point to reduce visual clutter. The
values in each row should add up to 1.0 or 100%.)


We follow the same three steps as before (see figure 3.12), except that we make a few
modifications in the code to compute all context vectors instead of only the second
one, z(2):

        attn_scores = torch.empty(6, 6)
        for i, x_i in enumerate(inputs):
        for j, x_j in enumerate(inputs):
        attn_scores[i, j] = torch.dot(x_i, x_j)
        print(attn_scores)

![image](https://github.com/user-attachments/assets/6d46ecba-b2e4-473e-a07b-c44a7b627ba9)
Figure 3.12 In step 1, we add an additional for loop to compute the dot
products for all pairs of inputs.


The resulting attention scores are as follows:

    tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],
    [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],
    [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],
    [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],
    [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],
    [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])

Each element in the tensor represents an attention score between each pair of inputs,
as we saw in figure 3.11.

Note that the values in that figure are normalized, which is why they differ from the unnormalized attention scores in the preceding tensor.

We will take care of the normalization later.
When computing the preceding attention score tensor, we used for loops in Python. 
However, for loops are generally slow, and we can achieve the same results using matrix multiplication:

    attn_scores = inputs @ inputs.T
    print(attn_scores)
    
We can visually confirm that the results are the same as before:

    tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],
    [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],
    [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],
    [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],
    [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],
    [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])

In step 2 of figure 3.12, we normalize each row so that the values in each row sum to 1:
    
    attn_weights = torch.softmax(attn_scores, dim=-1)
    print(attn_weights)
    
This returns the following attention weight tensor that matches the values shown in figure 3.10:

    tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],
    [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],
    [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],
    [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],
    [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],
    [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])

In the context of using PyTorch, the dim parameter in functions like torch.softmax
specifies the dimension of the input tensor along which the function will be computed.
By setting dim=-1, we are instructing the softmax function to apply the normalization
along the last dimension of the attn_scores tensor.

If attn_scores is a two-dimensional tensor (for example, with a shape of [rows, columns]), it will normalize
across the columns so that the values in each row (summing over the column dimension) sum up to 1.

We can verify that the rows indeed all sum to 1:

    row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
    print("Row 2 sum:", row_2_sum)
    print("All row sums:", attn_weights.sum(dim=-1))

The result is

    Row 2 sum: 1.0
    All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])
    
In the third and final step of figure 3.12, we use these attention weights to compute all context vectors via matrix multiplication:

    all_context_vecs = attn_weights @ inputs
    print(all_context_vecs)
    
In the resulting output tensor, each row contains a three-dimensional context vector:

    tensor([[0.4421, 0.5931, 0.5790],
    [0.4419, 0.6515, 0.5683],
    [0.4431, 0.6496, 0.5671],
    [0.4304, 0.6298, 0.5510],
    [0.4671, 0.5910, 0.5266],
    [0.4177, 0.6503, 0.5645]])

We can double-check that the code is correct by comparing the second row with the
context vector z(2) that we computed in section 3.3.1:

    print("Previous 2nd context vector:", context_vec_2)
    
Based on the result, we can see that the previously calculated context_vec_2 matches
the second row in the previous tensor exactly:

    Previous 2nd context vector:tensor([0.4419, 0.6515, 0.5683])
    
This concludes the code walkthrough of a simple self-attention mechanism.

Next, we will add trainable weights, enabling the LLM to learn from data and improve its performance
on specific tasks.
