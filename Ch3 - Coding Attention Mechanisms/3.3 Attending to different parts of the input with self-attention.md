# Self-attention 
We’ll now cover the inner workings of the self-attention mechanism and learn how to
code it from the ground up.

Self-attention serves as the cornerstone of every LLM based on the transformer architecture.
This topic may require a lot of focus and attention(no pun intended), but once you grasp its fundamentals, you will have conquered
one of the toughest aspects of this book and LLM implementation in general.

# The “self” in self-attention
In self-attention, the “self” refers to the mechanism’s ability to compute attention
weights by relating different positions within a single input sequence. 

It assesses and learns the relationships and dependencies between various parts of the input itself,
such as words in a sentence or pixels in an image.

This is in contrast to traditional attention mechanisms, where the focus is on the relationships
between elements of two different sequences, such as in sequence-tosequence
models where the attention might be between an input sequence and an
output sequence, such as the example depicted in figure 3.5.

Since self-attention can appear complex, especially if you are encountering it for the
first time, we will begin by examining a simplified version of it. Then we will implement
the self-attention mechanism with trainable weights used in LLMs.

# 3.3.1 A simple self-attention mechanism without trainable weights

Let’s begin by implementing a simplified variant of self-attention, free from any trainable
weights, as summarized in figure 3.7. 
The goal is to illustrate a few key concepts in self-attention before adding trainable weights.

![image](https://github.com/user-attachments/assets/eb796920-956f-4ae4-9d38-ea74c2989255)
Figure 3.7 The goal of self-attention is to compute a context vector for each input
element that combines information from all other input elements. In this example,
we compute the context vector z(2). The importance or contribution of each input
element for computing z(2) is determined by the attention weights 21 to 2T. When
computing z(2), the attention weights are calculated with respect to input element
x(2) and all other inputs.

----------------------------------------------------------------------------------------

Figure 3.7 shows an input sequence, denoted as x, consisting of T elements represented
as x(1) to x(T). 

This sequence typically represents text, such as a sentence, that has already been transformed into token embeddings.
For example, consider an input text like “Your journey starts with one step.” In this
case, each element of the sequence, such as x(1), corresponds to a d-dimensional
embedding vector representing a specific token, like “Your.” Figure 3.7 shows these
input vectors as three-dimensional embeddings.

In self-attention, our goal is to calculate [ context vectors z(i) ] for each element x(i)
in the input sequence. A context vector can be interpreted as an [ enriched embedding vector ].

To illustrate this concept, let’s focus on the embedding vector of the second input
element, x(2) (which corresponds to the token “journey”), and the corresponding context
vector, z(2), shown at the bottom of figure 3.7.

This enhanced context vector, z(2), is an embedding that contains information about x(2) and all other input elements,
x(1) to x(T).

###  [ Context vectors play a crucial role in self-attention ]. 

#### Their purpose is to create enriched representations of each element in an input sequence (like a sentence)
by incorporating information from all other elements in the sequence (figure 3.7).
This is essential in LLMs, which need to understand the relationship and relevance
of words in a sentence to each other.

Later, we will add trainable weights that help an LLM learn to construct these context vectors so that they are relevant for the
LLM to generate the next token. 

But first, let’s implement a simplified self-attention mechanism to compute these weights and the resulting context vector one
step at a time.

Consider the following input sentence, which has already been embedded into
three-dimensional vectors (see chapter 2). I’ve chosen a small embedding dimension
to ensure it fits on the page without line breaks:

    import torch
    inputs = torch.tensor(
              [[0.43, 0.15, 0.89], # Your (x^1)
              [0.55, 0.87, 0.66], # journey (x^2)
              [0.57, 0.85, 0.64], # starts (x^3)
              [0.22, 0.58, 0.33], # with (x^4)
              [0.77, 0.25, 0.10], # one (x^5)
              [0.05, 0.80, 0.55]] # step (x^6)
              )
### The first step of implementing self-attention is to compute the intermediate values ω,
referred to as attention scores, as illustrated in figure 3.8. Due to spatial constraints,
the figure displays the values of the preceding inputs tensor in a truncated version;
for example, 0.87 is truncated to 0.8. In this truncated version, the embeddings of the
words “journey” and “starts” may appear similar by random chance.

![image](https://github.com/user-attachments/assets/d0c3335f-2d80-4317-b9d3-d31da05e6e6d)
Figure 3.8 The overall goal is to illustrate the computation of the context vector z(2) using the
second input element, x(2) as a query. This figure shows the first intermediate step, computing the
attention scores  between the query x(2) and all other input elements as a dot product. (Note that
the numbers are truncated to one digit after the decimal point to reduce visual clutter.)

------------------------------------------------------------------------------------------


Figure 3.8 illustrates how we calculate the intermediate attention scores ω between the
query token and each input token. 
We determine these scores by computing the dot product of the query, x(2), with every other input token:


      query = inputs[1]
      attn_scores_2 = torch.empty(inputs.shape[0])
      for i, x_i in enumerate(inputs):
      attn_scores_2[i] = torch.dot(x_i, query)
      print(attn_scores_2)
      
The computed attention scores are

      tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])

## Understanding dot products
A dot product is essentially a concise way of multiplying two vectors element-wise and
then summing the products, which can be demonstrated as follows:

      res = 0.
      for idx, element in enumerate(inputs[0]):
      res += inputs[0][idx] * query[idx]
      print(res)
      print(torch.dot(inputs[0], query))

The output confirms that the sum of the element-wise multiplication gives the same
results as the dot product:
      
      tensor(0.9544)
      tensor(0.9544)

## 1. What is a Dot Product?
The dot product (or scalar product) is a mathematical operation that takes two vectors of the same length, multiplies their corresponding elements, and sums the results. Mathematically, for vectors a and b:

![image](https://github.com/user-attachments/assets/c2c38fa0-fb0d-4541-92b7-7972215131ce)

## 2. Code Example Explained

Your code computes the dot product manually using a loop and compares it to PyTorch’s torch.dot function. Let’s dissect this:

Manual Calculation (Loop):

    res = 0.
    for idx, element in enumerate(inputs[0]):
        res += inputs[0][idx] * query[idx]
    print(res)  # Output: sum(inputs[0][i] * query[i] for all i)


Goal: Iterate over each element of the two vectors (inputs[0] and query), multiply corresponding elements, and accumulate the sum.

## How it works:

  inputs[0] and query are vectors (1D tensors/arrays).
  
  For each index idx, multiply inputs[0][idx] and query[idx].
  
  Sum all these products into res.

PyTorch Equivalent:

    print(torch.dot(inputs[0], query))  # Same result as the loop!
    torch.dot() is an optimized, vectorized implementation of the dot product.

Why use it? Faster (uses hardware acceleration) and avoids manual loops.

## 3. Key Properties of Dot Products

### Measures Similarity:

  1. A large dot product indicates the vectors are "similar" in direction (aligned).
  
  2. A dot product of 0 means the vectors are orthogonal (perpendicular).
  
  3. Geometric Interpretation:
  
      a ⋅ b = ∥ a ∥ ∥ b ∥ cos (θ)

      Where θ is the angle between the vectors, and ∥a∥ is the magnitude (length) of a.

### Used Everywhere in ML:

  1. Neural networks (e.g., fully connected layers compute dot products).
      
  2. Attention mechanisms (queries and keys interact via dot products).
      
  3. Similarity metrics (e.g., cosine similarity).
