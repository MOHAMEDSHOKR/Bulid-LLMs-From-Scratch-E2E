Our next step will be to implement the self-attention mechanism used in the original
transformer architecture, the GPT models, and most other popular LLMs. This
self-attention mechanism is also called scaled dot-product attention. Figure 3.13 shows
how this self-attention mechanism fits into the broader context of implementing
an LLM.

![image](https://github.com/user-attachments/assets/80e40e3d-81c2-4314-9584-caa1d4a37a98)
Figure 3.13 Previously, we coded a simplified attention mechanism to understand the basic mechanism behind
attention mechanisms. Now, we add trainable weights to this attention mechanism. Later, we will extend this
self-attention mechanism by adding a causal mask and multiple heads.


As illustrated in figure 3.13, the self-attention mechanism with trainable weights builds
on the previous concepts: we want to compute context vectors as weighted sums over
the input vectors specific to a certain input element.

As you will see, there are only slight differences compared to the basic self-attention mechanism we coded earlier.
The most notable difference is the introduction of weight matrices that are
updated during model training.

These trainable weight matrices are crucial so that the model (specifically, the attention module inside the model) can learn to produce
“good” context vectors. (We will train the LLM in chapter 5.)

We will tackle this self-attention mechanism in the two subsections. First, we will code
it step by step as before. Second, we will organize the code into a compact Python class
that can be imported into the LLM architecture.

-----------------------------------------------------------------------------------------------------

## 3.4.1 Computing the attention weights step by step

We will implement the self-attention mechanism step by step by introducing the
three trainable weight matrices Wq, Wk, and Wv.

These three matrices are used to project the embedded input tokens, x(i), into query, key, and value vectors, respectively,
as illustrated in figure 3.14.

![image](https://github.com/user-attachments/assets/ef367fb9-d40d-4963-8b5b-878037e2b9df)
Figure 3.14 In the first step of the self-attention mechanism with trainable weight matrices, we compute query
(q), key (k), and value (v) vectors for input elements x. Similar to previous sections, we designate the second
input, x(2), as the query input. The query vector q(2) is obtained via matrix multiplication between the input x(2) and
the weight matrix Wq. Similarly, we obtain the key and value vectors via matrix multiplication involving the weight
matrices Wk and Wv.

Earlier, we defined the second input element x(2) as the query when we computed the
simplified attention weights to compute the context vector z(2). Then we generalized
this to compute all context vectors z(1) ... z(T) for the six-word input sentence “Your
journey starts with one step.”

Similarly, we start here by computing only one context vector, z(2), for illustration
purposes. We will then modify this code to calculate all context vectors.
Let’s begin by defining a few variables:

    x_2 = inputs[1] ---------------->The second input element
    d_in = inputs.shape[1] ----------------> The input embedding size, d=3
    d_out = 2 ----------------> The output embedding size, d_out=2


Note that in GPT-like models, the input and output dimensions are usually the same,
but to better follow the computation, we’ll use different input (d_in=3) and output
(d_out=2) dimensions here.

Next, we initialize the three weight matrices Wq, Wk, and Wv shown in figure 3.14:

    torch.manual_seed(123)
    W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
    W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
    W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
    
We set requires_grad=False to reduce clutter in the outputs, but if we were to use
the weight matrices for model training, we would set requires_grad=True to update
these matrices during model training.
Next, we compute the query, key, and value vectors:


    query_2 = x_2 @ W_query
    key_2 = x_2 @ W_key
    value_2 = x_2 @ W_value
    print(query_2)
    
The output for the query results in a two-dimensional vector since we set the number
of columns of the corresponding weight matrix, via d_out, to 2:

    tensor([0.4306, 1.4551])
    
### Weight parameters vs. attention weights

{ In the weight matrices W, the term “weight” is short for “weight parameters,” the values
of a neural network that are optimized during training. This is not to be confused
with the attention weights. As we already saw, attention weights determine the extent
to which a context vector depends on the different parts of the input (i.e., to what
extent the network focuses on different parts of the input).
In summary, weight parameters are the fundamental, learned coefficients that define
the network’s connections, while attention weights are dynamic, context-specific values.}


Even though our temporary goal is only to compute the one context vector, z(2), we still
require the key and value vectors for all input elements as they are involved in computing
the attention weights with respect to the query q(2) (see figure 3.14).

We can obtain all keys and values via matrix multiplication:

    keys = inputs @ W_key
    values = inputs @ W_value
    print("keys.shape:", keys.shape)
    print("values.shape:", values.shape)
    
As we can tell from the outputs, we successfully projected the six input tokens from a
three-dimensional onto a two-dimensional embedding space:
    
    keys.shape: torch.Size([6, 2])
    values.shape: torch.Size([6, 2])
    
The second step is to compute the attention scores, as shown in figure 3.15.

![image](https://github.com/user-attachments/assets/1c27eee5-7d92-4504-9ab4-e8f544478e8a)
Figure 3.15 The attention score computation is a dot-product computation similar to what we used in the
simplified self-attention mechanism in section 3.3. The new aspect here is that we are not directly computing the
dot-product between the input elements but using the query and key obtained by transforming the inputs via the
respective weight matrices.

First, let’s compute the attention score ω22:

    keys_2 = keys[1]
    attn_score_22 = query_2.dot(keys_2)
    print(attn_score_22)
    
The result for the unnormalized attention score is

    tensor(1.8524)
    
Again, we can generalize this computation to all attention scores via matrix
multiplication:

    attn_scores_2 = query_2 @ keys.T
    print(attn_scores_2)
    
As we can see, as a quick check, the second element in the output matches the
attn_score_22 we computed previously:

    tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])
    
Now, we want to go from the attention scores to the attention weights, as illustrated in
figure 3.16. We compute the attention weights by scaling the attention scores and
using the softmax function. However, now we scale the attention scores by dividing
them by the square root of the embedding dimension of the keys (taking the square
root is mathematically the same as exponentiating by 0.5):
    
    d_k = keys.shape[-1]
    attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)
    print(attn_weights_2)

![image](https://github.com/user-attachments/assets/045df214-7f7b-4cf2-9500-7728077dc308)
Figure 3.16 After computing the attention scores , the next step is to normalize these scores using the
softmax function to obtain the attention weights .

The resulting attention weights are

    tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])

## The rationale behind scaled-dot product attention
The reason for the normalization by the embedding dimension size is to improve the
training performance by avoiding small gradients.

For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large
dot products can result in very small gradients during backpropagation due to the
softmax function applied to them.

As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero.

These small gradients can drastically slow down learning or cause training to stagnate.
The scaling by the square root of the embedding dimension is the reason why this
self-attention mechanism is also called scaled-dot product attention.

Now, the final step is to compute the context vectors, as illustrated in figure 3.17.

![image](https://github.com/user-attachments/assets/679d6573-6af8-4083-878b-d5bbddcbeed9)
Figure 3.17 In the final step of the self-attention computation, we compute the context vector by combining all
value vectors via the attention weights.


Similar to when we computed the context vector as a weighted sum over the input vectors
(see section 3.3), we now compute the context vector as a weighted sum over the
value vectors.

Here, the attention weights serve as a weighting factor that weighs
the respective importance of each value vector. Also as before, we can use matrix multiplication
to obtain the output in one step:

    context_vec_2 = attn_weights_2 @ values
    print(context_vec_2)
    
The contents of the resulting vector are as follows:

    tensor([0.3061, 0.8210])
    
So far, we’ve only computed a single context vector, z(2). Next, we will generalize the
code to compute all context vectors in the input sequence, z(1) to z(T).


## Why query, key, and value?

The terms “key,” “query,” and “value” in the context of attention mechanisms are
borrowed from the domain of information retrieval and databases, where similar concepts
are used to store, search, and retrieve information.

1. A query is analogous to a search query in a database. It represents the current item
    (e.g., a word or token in a sentence) the model focuses on or tries to understand.
    The query is used to probe the other parts of the input sequence to determine how
    much attention to pay to them.

2. The key is like a database key used for indexing and searching. In the attention mechanism,
    each item in the input sequence (e.g., each word in a sentence) has an associated
    key. These keys are used to match the query.

3. The value in this context is similar to the value in a key-value pair in a database. It
    represents the actual content or representation of the input items. Once the model
    determines which keys (and thus which parts of the input) are most relevant to the
    query (the current focus item), it retrieves the corresponding values.

