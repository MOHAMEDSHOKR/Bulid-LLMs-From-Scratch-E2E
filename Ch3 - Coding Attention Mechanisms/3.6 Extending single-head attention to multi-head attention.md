#   Multi-Head Attention

Our final step will be to extend the previously implemented causal attention class over
multiple heads. This is also called " multi-head attention ".

The term “multi-head” refers to dividing the attention mechanism into multiple
“heads,” each operating independently. 

In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention
weights processing the input sequentially.

We will tackle this expansion from causal attention to multi-head attention.
 * First, we will intuitively build a multi-head attention module by stacking multiple Causal- Attention modules.
 * Then we will then implement the same multi-head attention module in a more complicated but more computationally efficient way.

## 3.6.1 Stacking multiple single-head attention layers

In practical terms, implementing multi-head attention involves creating multiple
instances of the self-attention mechanism (see figure 3.18), each with its own weights,
and then combining their outputs. Using multiple instances of the self-attention
mechanism can be computationally intensive, but it’s crucial for the kind of complex
pattern recognition that models like transformer-based LLMs are known for.

![image](https://github.com/user-attachments/assets/6fafed64-996f-408d-9b4c-4c6a4b2ed383)
Figure 3.18 In self-attention, we transform the input vectors in the input matrix X with the three weight
matrices, Wq, Wk, and Wv. The new compute the attention weight matrix based on the resulting queries (Q) and
keys (K). Using the attention weights and values (V), we then compute the context vectors (Z). For visual clarity,
we focus on a single input text with n tokens, not a batch of multiple inputs. Consequently, the three-dimensional
input tensor is simplified to a two-dimensional matrix in this context. This approach allows for a more straightforward
visualization and understanding of the processes involved. For consistency with later figures, the values in the
attention matrix do not depict the real attention weights. (The numbers in this figure are truncated to two digits
after the decimal point to reduce visual clutter. The values in each row should add up to 1.0 or 100%.)


----------------------------------------------------------------------------------------------


Figure 3.24 illustrates the structure of a multi-head attention module, which consists
of multiple single-head attention modules, as previously depicted in figure 3.18,
stacked on top of each other.


![image](https://github.com/user-attachments/assets/ed72a288-224e-4714-b268-18f9c21a068a)
Figure 3.24 The multi-head attention module includes two single-head attention modules stacked on top of
each other. So, instead of using a single matrix Wv for computing the value matrices, in a multi-head attention
module with two heads, we now have two value weight matrices: Wv1 and Wv2. The same applies to the other
weight matrices, WQ and Wk. We obtain two sets of context vectors Z1 and Z2 that we can combine into a single
context vector matrix Z.



-------------------------------------------------------------------------------------------

#  Main idea behind multi-head attention

As mentioned before, the main idea behind multi-head attention is to run the attention
mechanism multiple times (in parallel) with different, learned linear projections—the
results of multiplying the input data (like the query, key, and value vectors in attention
mechanisms) by a weight matrix. 

In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper class 
that stacks multiple instances of our previously implemented CausalAttention module.


### Listing 3.4 A wrapper class to implement multi-head attention

    class MultiHeadAttentionWrapper(nn.Module):
              def __init__(self, d_in, d_out, context_length,
                      dropout, num_heads, qkv_bias=False):
                      super().__init__()
                      self.heads = nn.ModuleList(
                      [CausalAttention(
                      d_in, d_out, context_length, dropout, qkv_bias
                      )
                      for _ in range(num_heads)]
                      )
                      
              def forward(self, x):
              
                      return torch.cat([head(x) for head in self.heads], dim=-1)


For example, if we use this MultiHeadAttentionWrapper class with two attention heads
(via num_heads=2) and CausalAttention output dimension d_out=2, we get a four dimensional
context vector (d_out*num_heads=4), as depicted in figure 3.25.

![image](https://github.com/user-attachments/assets/78097b2f-119d-4489-a38a-53ed83c1e043)
Figure 3.25 Using the MultiHeadAttentionWrapper, we specified the number of
attention heads (num_heads). If we set num_heads=2, as in this example, we obtain
a tensor with two sets of context vector matrices. In each context vector matrix, the
rows represent the context vectors corresponding to the tokens, and the columns
correspond to the embedding dimension specified via d_out=4. We concatenate these
context vector matrices along the column dimension. Since we have two attention
heads and an embedding dimension of 2, the final embedding dimension is 2 × 2 = 4.


----------------------------------------------------------------------------------------


To illustrate this further with a concrete example, we can use the MultiHeadAttention-
Wrapper class similar to the CausalAttention class before:

    torch.manual_seed(123)
    context_length = batch.shape[1] # This is the number of tokens
    d_in, d_out = 3, 2
    mha = MultiHeadAttentionWrapper(
    d_in, d_out, context_length, 0.0, num_heads=2)
    context_vecs = mha(batch)
    print(context_vecs)
    print("context_vecs.shape:", context_vecs.shape)
    
This results in the following tensor representing the context vectors:

    tensor([[[-0.4519, 0.2216, 0.4772, 0.1063],
            [-0.5874, 0.0058, 0.5891, 0.3257],
            [-0.6300, -0.0632, 0.6202, 0.3860],
            [-0.5675, -0.0843, 0.5478, 0.3589],
            [-0.5526, -0.0981, 0.5321, 0.3428],
            [-0.5299, -0.1081, 0.5077, 0.3493]],
            
            [[-0.4519, 0.2216, 0.4772, 0.1063],
            [-0.5874, 0.0058, 0.5891, 0.3257],
            [-0.6300, -0.0632, 0.6202, 0.3860],
            [-0.5675, -0.0843, 0.5478, 0.3589],
            [-0.5526, -0.0981, 0.5321, 0.3428],
            [-0.5299, -0.1081, 0.5077, 0.3493]]], grad_fn=<CatBackward0>)
            
    context_vecs.shape: torch.Size([2, 6, 4])


* The first dimension of the resulting context_vecs tensor is 2 since we have two input
    texts (the input texts are duplicated, which is why the context vectors are exactly the
    same for those).

* The second dimension refers to the 6 tokens in each input.

* The third dimension refers to the four-dimensional embedding of each token.


## Exercise 3.2 
Returning two-dimensional embedding vectors Change the input arguments for the MultiHeadAttentionWrapper(..., num_
heads=2) call such that the output context vectors are two-dimensional instead of
four dimensional while keeping the setting num_heads=2. Hint: You don’t have to
modify the class implementation; you just have to change one of the other input arguments.


Up to this point, we have implemented a MultiHeadAttentionWrapper that combined
multiple single-head attention modules. However, these are processed sequentially via
[head(x) for head in self.heads] in the forward method. We can improve this
implementation by processing the heads in parallel. One way to achieve this is by computing
the outputs for all attention heads simultaneously via matrix multiplication.
