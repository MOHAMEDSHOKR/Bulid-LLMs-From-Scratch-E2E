# Multi-Head Attention with weight splits

1. So far, we have created a MultiHeadAttentionWrapper to implement multi-head
    attention by stacking multiple single-head attention modules. This was done by instantiating
    and combining several CausalAttention objects.


2. Instead of maintaining two separate classes, MultiHeadAttentionWrapper and
    CausalAttention, we can combine these concepts into a single MultiHeadAttention
    class. Also, in addition to merging the MultiHeadAttentionWrapper with the Causal-
    Attention code, we will make some other modifications to implement multi-head
    attention more efficiently.


3. In the MultiHeadAttentionWrapper, multiple heads are implemented by creating
  a list of CausalAttention objects (self.heads), each representing a separate attention
  head. 

4. The CausalAttention class independently performs the attention mechanism,
    and the results from each head are concatenated. In contrast, the following
    MultiHeadAttention class integrates the multi-head functionality within a single class.


5. It splits the input into multiple heads by reshaping the projected query, key, and value
    tensors and then combines the results from these heads after computing attention.
    Let’s take a look at the MultiHeadAttention class before we discuss it further.

## Listing 3.5 An efficient multi-head attention class

       class MultiHeadAttention(nn.Module):
       
              def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
              
                      super().__init__()
                      assert (d_out % num_heads == 0), \
                      "d_out must be divisible by num_heads"
                      self.d_out = d_out
                      self.num_heads = num_heads
                      self.head_dim = d_out // num_heads --> Reduces the projection dim to match the desired output dim
                      self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
                      self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
                      self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
                      self.out_proj = nn.Linear(d_out, d_out) --> Uses a Linear layer to combine head outputs
                      self.dropout = nn.Dropout(dropout)
                      self.register_buffer(
                      "mask",
                      torch.triu(torch.ones(context_length, context_length),
                      diagonal=1))
                      
              def forward(self, x):
              
                      b, num_tokens, d_in = x.shape
                      keys = self.W_key(x)
                      queries = self.W_query(x)
                      values = self.W_value(x)
                      
                      keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) --> We implicitly plit the matrix y adding a num_heads dimension.
                                                                                        Then e unroll the last dim: (b, num_tokens, d_out) -> (b, num_tokens,num_heads,head_dim).
                      values = values.view(b, num_tokens, self.num_heads, self.head_dim)
                      queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)
                      
                      keys = keys.transpose(1, 2)
                      queries = queries.transpose(1, 2)--> Transposes from shape (b, num_tokens,num_heads, head_dim) to (b, num_heads,num_tokens, head_dim)
                      values = values.transpose(1, 2)
                      
                      attn_scores = queries @ keys.transpose(2, 3)--> Computes dot product for each head
                      mask_bool = self.mask.bool()[:num_tokens, :num_tokens] --> Masks truncated to the number of tokens
                      attn_scores.masked_fill_(mask_bool, -torch.inf) --> Uses the mask to fill attention scores
                      
                      attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
                      attn_weights = self.dropout(attn_weights)
                      
                      context_vec = (attn_weights @ values).transpose(1, 2) --> Tensor shape:(b, num_tokens,n_heads,head_dim)
                      context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
                      context_vec = self.out_proj(context_vec) --> Adds an optional linear projection
                      
                      return context_vec


Even though the reshaping (.view) and transposing (.transpose) of tensors inside
the MultiHeadAttention class looks very mathematically complicated, the Multi- HeadAttention class 
implements the same concept as the MultiHeadAttention-Wrapper earlier.


On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked
multiple single-head attention layers that we combined into a multi-head attention
layer. 

The MultiHeadAttention class takes an integrated approach.

It starts with a multi-head layer and then internally splits this layer into individual attention heads, as
illustrated in figure 3.26.


The splitting of the query, key, and value tensors is achieved through tensor reshaping
and transposing operations using PyTorch’s .view and .transpose methods. 

The input is first transformed (via linear layers for queries, keys, and values) and then
reshaped to represent multiple heads.


The key operation is to split the d_out dimension into num_heads and head_dim,
where head_dim = d_out / num_heads. This splitting is then achieved using the .view
method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension
(b, num_tokens, num_heads, head_dim).

![image](https://github.com/user-attachments/assets/872b84f0-c8bc-4f05-94a6-961fc44a3de1)
![image](https://github.com/user-attachments/assets/d8b49e29-4cd7-42e1-876a-feb430568026)
Figure 3.26 In the MultiHeadAttentionWrapper class with two attention heads,
we initialized two weight matrices, Wq1 and Wq2, and computed two query matrices, Q1
and Q2 (top). In the MultiheadAttention class, we initialize one larger weight matrix
Wq, only perform one matrix multiplication with the inputs to obtain a query matrix Q, and
then split the query matrix into Q1 and Q2 (bottom). We do the same for the keys and
values, which are not shown to reduce visual clutter.


-----------------------------------------------------------------------------------------------------

The tensors are then transposed to bring the num_heads dimension before the num_ tokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim).

This transposition is crucial for correctly aligning the queries, keys, and values across the
different heads and performing batched matrix multiplications efficiently.
To illustrate this batched matrix multiplication, suppose we have the following
tensor:
      
      a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],
                          [0.8993, 0.0390, 0.9268, 0.7388],
                          [0.7179, 0.7058, 0.9156, 0.4340]],  --> The shape of thisbtensor is (b, num_heads,num_tokens, head_dim) = (1, 2, 3, 4).
                          [[0.0772, 0.3565, 0.1479, 0.5331],
                          [0.4066, 0.2318, 0.4545, 0.9737],
                          [0.4606, 0.5159, 0.4220, 0.5786]]]])


Now we perform a batched matrix multiplication between the tensor itself and a view
of the tensor where we transposed the last two dimensions, num_tokens and head_dim:

    print(a @ a.transpose(2, 3))
    
The result is

    tensor([[[[1.3208, 1.1631, 1.2879],
              [1.1631, 2.2150, 1.8424],
              [1.2879, 1.8424, 2.0402]],
              [[0.4391, 0.7003, 0.5903],
              [0.7003, 1.3737, 1.0620],
              [0.5903, 1.0620, 0.9912]]]])

In this case, the matrix multiplication implementation in PyTorch handles the fourdimensional
input tensor so that the matrix multiplication is carried out between the two
last dimensions (num_tokens, head_dim) and then repeated for the individual heads.


For instance, the preceding becomes a more compact way to compute the matrix
multiplication for each head separately:

      first_head = a[0, 0, :, :]
      first_res = first_head @ first_head.T
      print("First head:\n", first_res)
      
      second_head = a[0, 1, :, :]
      second_res = second_head @ second_head.T
      print("\nSecond head:\n", second_res)

The results are exactly the same results as those we obtained when using the batched
matrix multiplication print(a @ a.transpose(2, 3)):

    First head:
    tensor([[1.3208, 1.1631, 1.2879],
    [1.1631, 2.2150, 1.8424],
    [1.2879, 1.8424, 2.0402]])
    
    Second head:
    tensor([[0.4391, 0.7003, 0.5903],
    [0.7003, 1.3737, 1.0620],
    [0.5903, 1.0620, 0.9912]])

    
Continuing with MultiHeadAttention, after computing the attention weights and context
vectors, the context vectors from all heads are transposed back to the shape (b,
num_tokens, num_heads, head_dim). 

These vectors are then reshaped (flattened) into the shape (b, num_tokens, d_out), effectively combining the outputs from all heads.
Additionally, we added an output projection layer (self.out_proj) to Multi- HeadAttention after combining the heads, which is not present in the Causal-
Attention class. 

This output projection layer is not strictly necessary (see appendix B formore details), but it is commonly used in many LLM architectures, which is why I
added it here for completeness.

Even though the MultiHeadAttention class looks more complicated than the
MultiHeadAttentionWrapper due to the additional reshaping and transposition of
tensors, it is more efficient.

The reason is that we only need one matrix multiplication
to compute the keys, for instance, keys = self.W_key(x) (the same is true for the queries
and values). 

In the MultiHeadAttentionWrapper, we needed to repeat this matrix
multiplication, which is computationally one of the most expensive steps, for each
attention head.

The MultiHeadAttention class can be used similar to the SelfAttention and
CausalAttention classes we implemented earlier:
    
    torch.manual_seed(123)
    batch_size, context_length, d_in = batch.shape
    d_out = 2
    mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)
    context_vecs = mha(batch)
    print(context_vecs)
    print("context_vecs.shape:", context_vecs.shape)

    
The results show that the output dimension is directly controlled by the d_out
argument:

    tensor([[[0.3190, 0.4858],
            [0.2943, 0.3897],
            [0.2856, 0.3593],
            [0.2693, 0.3873],
            [0.2639, 0.3928],
            [0.2575, 0.4028]],
            
            [[0.3190, 0.4858],
            [0.2943, 0.3897],
            [0.2856, 0.3593],
            [0.2693, 0.3873],
            [0.2639, 0.3928],
            [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)
            
    context_vecs.shape: torch.Size([2, 6, 2])


We have now implemented the MultiHeadAttention class that we will use when we
implement and train the LLM. Note that while the code is fully functional, I used
relatively small embedding sizes and numbers of attention heads to keep the outputs
readable.

For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention
heads and a context vector embedding size of 768. The largest GPT-2 model (1.5
billion parameters) has 25 attention heads and a context vector embedding size of
1,600. The embedding sizes of the token inputs and context embeddings are the same
in GPT models   (d_in = d_out).

## Exercise 3.3
Initializing GPT-2 size attention modules Using the MultiHeadAttention class, initialize a multi-head attention module that
has the same number of attention heads as the smallest GPT-2 model (12 attention
heads). Also ensure that you use the respective input and output embedding sizes
similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a
context length of 1,024 tokens.


## Summary

1. Attention mechanisms transform input elements into enhanced context vector
    representations that incorporate information about all inputs.

2. A self-attention mechanism computes the context vector representation as a
    weighted sum over the inputs.
   
4. In a simplified attention mechanism, the attention weights are computed via
    dot products.
   
6. A dot product is a concise way of multiplying two vectors element-wise and then
    summing the products.
   
8. Matrix multiplications, while not strictly required, help us implement computations
    more efficiently and compactly by replacing nested for loops.
   
10. In self-attention mechanisms used in LLMs, also called scaled-dot product
  attention, we include trainable weight matrices to compute intermediate transformations
  of the inputs: queries, values, and keys.

12. When working with LLMs that read and generate text from left to right, we add
    a causal attention mask to prevent the LLM from accessing future tokens.
    
14. In addition to causal attention masks to zero-out attention weights, we can add
    a dropout mask to reduce overfitting in LLMs.
    
16. The attention modules in transformer-based LLMs involve multiple instances of
  causal attention, which is called multi-head attention.

18. We can create a multi-head attention module by stacking multiple instances of
    causal attention modules.
    
20. A more efficient way of creating multi-head attention modules involves batched
    matrix multiplications.
