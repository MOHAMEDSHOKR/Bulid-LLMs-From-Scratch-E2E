## RNN 
Although RNNs work fine for translating short sentences, they don’t work well for longer
texts as they don’t have direct access to previous words in the input. One major
shortcoming in this approach is that the RNN must remember the entire encoded
input in a single hidden state before passing it to the decoder (figure 3.4).

![RNN Graph](https://github.com/user-attachments/assets/9c15c58c-8c0f-470c-92d1-730b58bc0116) ![RNN Model](https://github.com/user-attachments/assets/761e876a-e63a-4b4f-a695-47f5986cfd21)

Hence, researchers developed the " [ Bahdanau attention mechanism ] " for RNNs in
2014 (named after the first author of the respective paper; for more information, see
appendix B), which modifies the encoder–decoder RNN such that the decoder can
selectively access different parts of the input sequence at each decoding step as illustrated
in figure 3.5.

![image](https://github.com/user-attachments/assets/e7fed09f-82ac-4b8b-983d-44c8c2ac57bf)
Figure 3.5 Using an attention mechanism, the text-generating decoder part of the network can
access all input tokens selectively. This means that some input tokens are more important than others
for generating a given output token. The importance is determined by the attention weights, which we
will compute later. Note that this figure shows the general idea behind attention and does not depict
the exact implementation of the Bahdanau mechanism, which is an RNN method outside this book’s
scope.

--------------------------------------------------------------------------------------------------------

Interestingly, only three years later, researchers found that RNN architectures are
not required for building deep neural networks for natural language processing and
proposed the original transformer architecture (discussed in chapter 1) including a
self-attention mechanism inspired by the Bahdanau attention mechanism.

![Trans](https://github.com/user-attachments/assets/de832317-1f80-4a21-bc65-2b23159184a0)
## Self-attention 
is a mechanism that allows each position in the input sequence to
consider the relevancy of, or “attend to,” all other positions in the same sequence
when computing the representation of a sequence.

## Self-attention is a key component
of contemporary LLMs based on the [ transformer architecture ], such as the GPT series.
This chapter focuses on coding and understanding this self-attention mechanism
used in GPT-like models, as illustrated in figure 3.6. In the next chapter, we will code
the remaining parts of the LLM.

![image](https://github.com/user-attachments/assets/962e5eeb-f73b-431a-b044-62d66f219323)
Figure 3.6 Self-attention is a mechanism in transformers used to compute
more efficient input representations by allowing each position in a sequence to
interact with and weigh the importance of all other positions within the same
sequence. In this chapter, we will code this self-attention mechanism from the
ground up before we code the remaining parts of the GPT-like LLM in the
following chapter.

---------------------------------------------------------------------------------------------



![Transformers M](https://github.com/user-attachments/assets/2d1d076c-9a66-401f-84e1-f9a705afb901)
