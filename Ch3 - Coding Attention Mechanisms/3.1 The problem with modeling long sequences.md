Before we dive into the self-attention mechanism at the heart of LLMs, let’s consider
the problem with pre-LLM architectures that do not include attention mechanisms.
Suppose we want to develop a language translation model that translates text from
one language into another. As shown in figure 3.3, we can’t simply translate a text word
by word due to the grammatical structures in the source and target language.

![image](https://github.com/user-attachments/assets/25ecaf66-d947-428a-81ea-e5aa26242160)
![image](https://github.com/user-attachments/assets/6c360900-22c3-4be1-82d8-cc2b690d7838)
Figure 3.3 When translating text from one language to another, such as German to English, it’s not
possible to merely translate word by word. Instead, the translation process requires contextual
understanding and grammatical alignment.

------------------------------------------------------------------------------------------------------

To address this problem, it is common to use a deep neural network with two submodules,
an encoder and a decoder. The job of the encoder is to first read in and process the
entire text, and the decoder then produces the translated text.
Before the advent of transformers," recurrent neural networks (RNNs) "were the most
popular encoder–decoder architecture for language translation. An RNN is a type of
neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text.

If you are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed workings of RNNs to follow
this discussion; our focus here is more on the general concept of the encoder–decoder setup.

### In an encoder–decoder RNN, 
the input text is fed into the encoder, which processes it sequentially.
The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence
in the final hidden state, as illustrated in figure 3.4. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. It
also updates its hidden state at each step, which is supposed to carry the context necessary
for the next-word prediction.

![image](https://github.com/user-attachments/assets/f9ddb8d5-9304-4d22-b17b-e9167c66f463)
Figure 3.4 Before the advent of transformer models, encoder–decoder RNNs were a popular choice
for machine translation. The encoder takes a sequence of tokens from the source language as input,
where a hidden state (an intermediate neural network layer) of the encoder encodes a compressed
representation of the entire input sequence. Then, the decoder uses its current hidden state to begin
the translation, token by token.


![RNN](https://github.com/user-attachments/assets/cdbc5263-506d-47f9-8e55-e3bf789598d7)

