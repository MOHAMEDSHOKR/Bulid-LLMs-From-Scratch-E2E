Before we dive into the self-attention mechanism at the heart of LLMs, let’s consider
the problem with pre-LLM architectures that do not include attention mechanisms.
Suppose we want to develop a language translation model that translates text from
one language into another. As shown in figure 3.3, we can’t simply translate a text word
by word due to the grammatical structures in the source and target language.

![image](https://github.com/user-attachments/assets/25ecaf66-d947-428a-81ea-e5aa26242160)
![image](https://github.com/user-attachments/assets/6c360900-22c3-4be1-82d8-cc2b690d7838)
Figure 3.3 When translating text from one language to another, such as German to English, it’s not
possible to merely translate word by word. Instead, the translation process requires contextual
understanding and grammatical alignment.

------------------------------------------------------------------------------------------------------

To address this problem, it is common to use a deep neural network with two submodules,
an encoder and a decoder. The job of the encoder is to first read in and process the
entire text, and the decoder then produces the translated text.
Before the advent of transformers," recurrent neural networks (RNNs) "were the most
popular encoder–decoder architecture for language translation. An RNN is a type of
neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text.

If you are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed workings of RNNs to follow
this discussion; our focus here is more on the general concept of the encoder–decoder setup.

### In an encoder–decoder RNN, 
the input text is fed into the encoder, which processes it sequentially.
The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence
in the final hidden state, as illustrated in figure 3.4. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. It
also updates its hidden state at each step, which is supposed to carry the context necessary
for the next-word prediction.

![image](https://github.com/user-attachments/assets/f9ddb8d5-9304-4d22-b17b-e9167c66f463)
Figure 3.4 Before the advent of transformer models, encoder–decoder RNNs were a popular choice
for machine translation. The encoder takes a sequence of tokens from the source language as input,
where a hidden state (an intermediate neural network layer) of the encoder encodes a compressed
representation of the entire input sequence. Then, the decoder uses its current hidden state to begin
the translation, token by token.


![RNN](https://github.com/user-attachments/assets/cdbc5263-506d-47f9-8e55-e3bf789598d7)

An encoder-decoder RNN is a neural network architecture designed for sequence-to-sequence (seq2seq) tasks, where the input and output are sequences of potentially different lengths. Common applications include machine translation, text summarization, and speech recognition. Here's a breakdown of how it works:



### 1. Core Components
   
### Encoder:

  * Processes the input sequence (e.g., a sentence in French) into a compact representation called the context vector.
  
  * Typically uses an RNN variant (e.g., LSTM or GRU) to handle sequential data. At each step, it updates its hidden state based on the current input and previous state.
  
  * The final hidden state of the encoder captures the input's semantic meaning and serves as the initial context for the decoder.

### Decoder:

   * Generates the output sequence (e.g., the translated English sentence) step-by-step, conditioned on the context vector.

   * Also an RNN, initialized with the encoder’s final hidden state. At each step, it uses:

   * Its current hidden state.

   * The previous predicted token (or the ground-truth token during training, via teacher forcing).

   * Produces a probability distribution over the vocabulary for the next token.

### 2. Workflow

### Encoding:

   * Input tokens (e.g., words) are fed sequentially into the encoder.

   * The encoder’s hidden state evolves until the entire input is processed.

   * The final hidden state becomes the context vector (e.g., a fixed-size vector summarizing the input).

### Decoding:

   * The decoder starts with the context vector and a special <START> token.

   * At each step, it predicts the next token (e.g., a word) using:

   * The current hidden state.

   * The previous output (or ground-truth token during training).

   * The process continues until a <END> token is generated or a maximum length is reached.

### 3. Training

   * Loss Function: Cross-entropy loss is calculated at each decoder step, comparing predicted tokens to the target sequence.

   * Teacher Forcing: During training, the decoder uses the ground-truth previous token (instead of its own prediction) to stabilize learning.

   * Backpropagation: Gradients flow through both the decoder and encoder, updating all weights.

### 4. Limitations & Enhancements
   
   * Information Bottleneck: The fixed-size context vector may lose details from long sequences.

   * Solution: Attention mechanisms allow the decoder to dynamically focus on relevant parts of the input (e.g., Transformer models).

   * Vanishing Gradients: Addressed using LSTM/GRU cells instead of vanilla RNNs.

### 5. Example: Machine Translation
   
   * Input (Encoder): "Je suis étudiant" (French).

   * Context Vector: Encoder’s final hidden state.

   * Output (Decoder): "I am a student" (English), generated token-by-token.

### 6. Key Applications
   
   * Machine translation.

   * Text summarization.

   * Speech-to-text.

  * Dialogue systems.

### Summary

The encoder-decoder RNN maps variable-length input to output sequences by compressing input into a context vector and decoding it step-by-step. While foundational, modern variants (e.g., with attention) address its limitations for complex tasks.
