# 2.3 Converting tokens into token IDs
Next, let’s convert these tokens from a Python string to an integer representation to produce the token IDs. 
This conversion is an intermediate step before converting the token IDs into embedding vectors.
To map the previously generated tokens into token IDs, we have to build a vocabulary
first. This vocabulary defines how we map each unique word and special character
to a unique integer, as shown in figure 2.6.

![image](https://github.com/user-attachments/assets/aed5f8ee-72b0-46f4-9009-e4ea2a6d1882)

Figure 2.6 We build a vocabulary by tokenizing the entire text in a training dataset into individual
tokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed.
The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique
token to a unique integer value. The depicted vocabulary is purposefully small and contains no
punctuation or special characters for simplicity.
______________________________________________________________________________________________________

Now that we have tokenized Edith Wharton’s short story and assigned it to a Python
variable called preprocessed, let’s create a list of all unique tokens and sort them
alphabetically to determine the vocabulary size:

    all_words = sorted(set(preprocessed))
    vocab_size = len(all_words)
    print(vocab_size)

After determining that the vocabulary size is 1,130 via this code, we create the vocabulary
and print its first 51 entries for illustration purposes.

### Listing 2.2 Creating a vocabulary

    vocab = {token:integer for integer,token in enumerate(all_words)}
    for i, item in enumerate(vocab.items()):
    print(item)
    if i >= 50:
    break
    
The output is

        ('!', 0)
        ('"', 1)
        ("'", 2)
        ...
        ('Her', 49)
        ('Hermia', 50)


As we can see, the dictionary contains individual tokens associated with unique integer
labels. Our next goal is to apply this vocabulary to convert new text into token IDs
(figure 2.7).

![image](https://github.com/user-attachments/assets/c85a7721-468b-4508-a138-2fc7e059cdcb)
Figure 2.7 Starting with a new text sample, we tokenize the text and use the vocabulary to convert
the text tokens into token IDs. The vocabulary is built from the entire training set and can be applied
to the training set itself and any new text samples. The depicted vocabulary contains no punctuation
or special characters for simplicity.
______________________________________________________________________________________________________

When we want to convert the outputs of an LLM from numbers back into text, we need a
way to turn token IDs into text. For this, we can create an inverse version of the vocabulary
that maps token IDs back to the corresponding text tokens.

Let’s implement a complete tokenizer class in Python with an encode method that
splits text into tokens and carries out the string-to-integer mapping to produce token
IDs via the vocabulary. In addition, we’ll implement a decode method that carries out
the reverse integer-to-string mapping to convert the token IDs back into text. The following
listing shows the code for this tokenizer implementation.

### Listing 2.3 Implementing a simple text tokenizer

    class SimpleTokenizerV1:
    
          def __init__(self, vocab):
          
                self.str_to_int = vocab  # Stores the vocabulary as a class attribute for access in the encode and decode methods
                self.int_to_str = {i:s for s,i in vocab.items()} # Creates an inverse vocabulary that maps token IDs back to the original text tokens
                
          def encode(self, text):  # Processes input text into token IDs
          
                preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)
                preprocessed = [
                item.strip() for item in preprocessed if item.strip()
                ]
                ids = [self.str_to_int[s] for s in preprocessed]
                return ids
                
          def decode(self, ids):  # Converts token IDs back into text
          
                text = " ".join([self.int_to_str[i] for i in ids])
                text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)  # Removes spaces before the specified punctuation
                return text
                
Using the SimpleTokenizerV1 Python class, we can now instantiate new tokenizer
objects via an existing vocabulary, which we can then use to encode and decode text,
as illustrated in figure 2.8.

Let’s instantiate a new tokenizer object from the SimpleTokenizerV1 class and
tokenize a passage from Edith Wharton’s short story to try it out in practice:

    tokenizer = SimpleTokenizerV1(vocab)
    text = """"It's the last he painted, you know,"
    Mrs. Gisburn said with pardonable pride."""
    ids = tokenizer.encode(text)
    print(ids)
The preceding code prints the following token IDs:

    [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108,
    754, 793, 7]

Next, let’s see whether we can turn these token IDs back into text using the decode
method:

    print(tokenizer.decode(ids))


![image](https://github.com/user-attachments/assets/ef69ceb2-0f8d-4eeb-b5a2-4f338b6e0ba3)
Figure 2.8 Tokenizer implementations share two common methods: an encode method and a decode
method. The encode method takes in the sample text, splits it into individual tokens, and converts the
tokens into token IDs via the vocabulary. The decode method takes in token IDs, converts them back
into text tokens, and concatenates the text tokens into natural text.

This outputs:

    '" It\' s the last he painted, you know," Mrs. Gisburn said with
    pardonable pride.'

Based on this output, we can see that the decode method successfully converted the
token IDs back into the original text.

So far, so good. We implemented a tokenizer capable of tokenizing and detokenizing
text based on a snippet from the training set. Let’s now apply it to a new text sample
not contained in the training set:

    text = "Hello, do you like tea?"
    print(tokenizer.encode(text))
    
Executing this code will result in the following error:

    KeyError: 'Hello'

The problem is that the word “Hello” was not used in the “The Verdict” short story.
Hence, it is not contained in the vocabulary. This highlights the need to consider
large and diverse training sets to extend the vocabulary when working on LLMs.

Next, we will test the tokenizer further on text that contains unknown words and
discuss additional special tokens that can be used to provide further context for an
LLM during training.



