# This chapter covers 

## 1- Preparing text for large language model training.

## 2- Splitting text into word and subword tokens.

## 3- Byte pair encoding as a more advanced way of tokenizing text.

## 4- Sampling training examples with a sliding window approach.

## 5- Converting tokens into vectors that feed into a large language model.
