    # 2.5 Byte pair encoding

Let’s look at a more sophisticated tokenization scheme based on a concept called byte
pair encoding (BPE). The BPE tokenizer was used to train LLMs such as GPT-2, GPT-3,
and the original model used in ChatGPT.

Since implementing BPE can be relatively complicated, we will use an existing
Python open source library called tiktoken (https://github.com/openai/tiktoken), which
implements the BPE algorithm very efficiently based on source code in Rust. Similar
to other Python libraries, we can install the tiktoken library via Python’s pip installer
from the terminal:

    !pip install tiktoken

The code we will use is based on tiktoken 0.7.0. You can use the following code to
check the version you currently have installed:


    from importlib.metadata import version
    import tiktoken
    
    print("tiktoken version:", version("tiktoken"))
    
Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:

    tokenizer = tiktoken.get_encoding("gpt2")
    
The usage of this tokenizer is similar to the SimpleTokenizerV2 we implemented previously
    via an encode method:
    
    text = (
    "Hello, do you like tea? <|endoftext|> In the sunlit terraces"
    "of someunknownPlace."
    )
    integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})
    print(integers)

The code prints the following token IDs:

    [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250,
    8812, 2114, 286, 617, 34680, 27271, 13]

We can then convert the token IDs back into text using the decode method, similar to
our SimpleTokenizerV2:

    strings = tokenizer.decode(integers)
    print(strings)

The code prints

    Hello, do you like tea? <|endoftext|> In the sunlit terraces of
    someunknownPlace.
We can make two noteworthy observations based on the token IDs and decoded text.

First, the <|endoftext|> token is assigned a relatively large token ID, namely, 50256.

In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and
the original model used in ChatGPT, has a total vocabulary size of 50,257, with
<|endoftext|> being assigned the largest token ID.

Second, the BPE tokenizer encodes and decodes unknown words, such as
someunknownPlace, correctly. The BPE tokenizer can handle any unknown word.

How does it achieve this without using <|unk|> tokens?

The algorithm underlying BPE breaks down words that aren’t in its predefined
vocabulary into smaller subword units or even individual characters, enabling it to
handle out-of-vocabulary words. 

So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization,
it can represent it as a sequence of subword tokens or characters, as illustrated in figure 2.11.

![image](https://github.com/user-attachments/assets/314b576d-5e9c-4d2a-bc2e-933eff929916)
Figure 2.11 BPE tokenizers break down unknown words into subwords and individual characters. This way, a BPE
tokenizer can parse any word and doesn’t need to replace unknown words with special tokens, such as <|unk|>.

The ability to break down unknown words into individual characters ensures that
the tokenizer and, consequently, the LLM that is trained with it can process any text,
even if it contains words that were not present in its training data.

Exercise 2.1 Byte pair encoding of unknown words
Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and
print the individual token IDs. Then, call the decode function on each of the resulting
integers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the
decode method on the token IDs to check whether it can reconstruct the original
input, “Akwirw ier.”

A detailed discussion and implementation of BPE is out of the scope of this book, but
in short, it builds its vocabulary by iteratively merging frequent characters into subwords
and frequent subwords into words. 

For example, BPE starts with adding all individual single characters to its vocabulary (“a,” “b,” etc.).
In the next stage, it merges character combinations that frequently occur together into subwords. For example,
“d” and “e” may be merged into the subword “de,” which is common in many English
words like “define,” “depend,” “made,” and “hidden.” The merges are determined by
a frequency cutoff.

