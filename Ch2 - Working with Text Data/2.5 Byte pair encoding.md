# 2.5 Byte pair encoding

Let’s look at a more sophisticated tokenization scheme based on a concept called byte
pair encoding (BPE). The BPE tokenizer was used to train LLMs such as GPT-2, GPT-3,
and the original model used in ChatGPT.

Since implementing BPE can be relatively complicated, we will use an existing
Python open source library called tiktoken (https://github.com/openai/tiktoken), which
implements the BPE algorithm very efficiently based on source code in Rust. Similar
to other Python libraries, we can install the tiktoken library via Python’s pip installer
from the terminal:

    !pip install tiktoken

The code we will use is based on tiktoken 0.7.0. You can use the following code to
check the version you currently have installed:


    from importlib.metadata import version
    import tiktoken
    
    print("tiktoken version:", version("tiktoken"))
    
Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:

    tokenizer = tiktoken.get_encoding("gpt2")
    
The usage of this tokenizer is similar to the SimpleTokenizerV2 we implemented previously
    via an encode method:
    
    text = (
    "Hello, do you like tea? <|endoftext|> In the sunlit terraces"
    "of someunknownPlace."
    )
    integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})
    print(integers)
