# 2.6 Data sampling with a sliding window

The next step in creating the embeddings for the LLM is to generate the input–target
pairs required for training an LLM. What do these input–target pairs look like? As we
already learned, LLMs are pretrained by predicting the next word in a text, as depicted
in figure 2.12.

![image](https://github.com/user-attachments/assets/976a736f-f434-40c6-8e29-543684722d62)
Figure 2.12 Given a text sample, extract input blocks as subsamples that serve as
input to the LLM, and the LLM’s prediction task during training is to predict the next
word that follows the input block. During training, we mask out all words that are past
the target. Note that the text shown in this figure must undergo tokenization before
the LLM can process it; however, this figure omits the tokenization step for clarity.

___________________________________________________________________________________________________

Let’s implement a data loader that fetches the input–target pairs in figure 2.12 from
the training dataset using a sliding window approach. To get started, we will tokenize
the whole “The Verdict” short story using the BPE tokenizer:

    with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
    enc_text = tokenizer.encode(raw_text)
    print(len(enc_text))

Executing this code will return 5145, the total number of tokens in the training set,
after applying the BPE tokenizer.
Next, we remove the first 50 tokens from the dataset for demonstration purposes,
as it results in a slightly more interesting text passage in the next steps:

    enc_sample = enc_text[50:]
    
One of the easiest and most intuitive ways to create the input–target pairs for the nextword
prediction task is to create two variables, x and y, where x contains the input
tokens and y contains the targets, which are the inputs shifted by 1:

    context_size = 4
    x = enc_sample[:context_size]
    y = enc_sample[1:context_size+1]
    print(f"x: {x}")
    print(f"y: {y}")

Running the previous code prints the following output:

    x: [290, 4920, 2241, 287]
    y: [4920, 2241, 287, 257]

By processing the inputs along with the targets, which are the inputs shifted by one
position, we can create the next-word prediction tasks (see figure 2.12), as follows:

    for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(context, "---->", desired)
    
The code prints
    
    [290] ----> 4920
    [290, 4920] ----> 2241
    [290, 4920, 2241] ----> 287
    [290, 4920, 2241, 287] ----> 257

Everything left of the arrow (---->) refers to the input an LLM would receive, and
the token ID on the right side of the arrow represents the target token ID that the
LLM is supposed to predict. Let’s repeat the previous code but convert the token IDs
into text:
    
    for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(tokenizer.decode(context), "---->", tokenizer.decode([desired]))

The following outputs show how the input and outputs look in text format:

    and ----> established
    and established ----> himself
    and established himself ----> in
    and established himself in ----> a
    
We’ve now created the input–target pairs that we can use for LLM training.
There’s only one more task before we can turn the tokens into embeddings: implementing
an efficient data loader that iterates over the input dataset and returns the inputs 
and targets as PyTorch tensors, which can be thought of as multidimensional
arrays. In particular, we are interested in returning two tensors: an input tensor containing
the text that the LLM sees and a target tensor that includes the targets for the
LLM to predict, as depicted in figure 2.13. While the figure shows the tokens in string
format for illustration purposes, the code implementation will operate on token IDs
directly since the encode method of the BPE tokenizer performs both tokenization
and conversion into token IDs as a single step.

![image](https://github.com/user-attachments/assets/5b28b7b4-318f-40c9-ae83-f4932c236fb1)
Figure 2.13 To implement efficient data loaders, we collect the inputs in a tensor, x, where each row
represents one input context. A second tensor, y, contains the corresponding prediction targets (next
words), which are created by shifting the input by one position.

### NOTE 
For the efficient data loader implementation, we will use PyTorch’s
built-in Dataset and DataLoader classes. For additional information and
guidance on installing PyTorch, please see section A.2.1.3 in appendix A.
The code for the dataset class is shown in the following listing.

### Listing 2.5 A dataset for batched inputs and targets

    import torch
    
    from torch.utils.data import Dataset, DataLoader
    
    class GPTDatasetV1(Dataset):
    
        def __init__(self, txt, tokenizer, max_length, stride):
        
              self.input_ids = []
              self.target_ids = []
              token_ids = tokenizer.encode(txt)  # Tokenizes the entire text
              for i in range(0, len(token_ids) - max_length, stride): # Uses a sliding window to chunk the book into overlapping sequences of max_length
              input_chunk = token_ids[i:i + max_length]
              target_chunk = token_ids[i + 1: i + max_length + 1]
              self.input_ids.append(torch.tensor(input_chunk))
              self.target_ids.append(torch.tensor(target_chunk))
              
        def __len__(self): # Returns the total number of rows in the dataset
        
              return len(self.input_ids)
              
        def __getitem__(self, idx): # Returns a single row from the dataset
        
              return self.input_ids[idx], self.target_ids[idx]

The GPTDatasetV1 class is based on the PyTorch Dataset class and defines how individual
rows are fetched from the dataset, where each row consists of a number of
token IDs (based on a max_length) assigned to an input_chunk tensor.

The target_chunk tensor contains the corresponding targets. I recommend reading on to see what
the data returned from this dataset looks like when we combine the dataset with a
PyTorch DataLoader—this will bring additional intuition and clarity.

### NOTE
If you are new to the structure of PyTorch Dataset classes, such as
shown in listing 2.5, refer to section A.6 in appendix A, which explains the
general structure and usage of PyTorch Dataset and DataLoader classes.
The following code uses the GPTDatasetV1 to load the inputs in batches via a PyTorch
DataLoader.

### Listing 2.6 A data loader to generate batches with input-with pairs
    
    def create_dataloader_v1(txt, batch_size=4, max_length=256,
    stride=128, shuffle=True, drop_last=True,
    num_workers=0):
    
    tokenizer = tiktoken.get_encoding("gpt2") # Initializes the tokenizer
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # Creates dataset
    dataloader = DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=shuffle,
    drop_last=drop_last, # drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.
    num_workers=num_workers # The number of CPU processes to use for preprocessing
    )
    return dataloader

Let’s test the dataloader with a batch size of 1 for an LLM with a context size of 4 to
develop an intuition of how the GPTDatasetV1 class from listing 2.5 and the create_dataloader_v1 function from listing 2.6 work together:

    with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
    dataloader = create_dataloader_v1(
    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)
    data_iter = iter(dataloader)
    first_batch = next(data_iter)
    print(first_batch)
    
Executing the preceding code prints the following:

    [tensor([[ 40, 367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]

The first_batch variable contains two tensors: the first tensor stores the input token
IDs, and the second tensor stores the target token IDs. Since the max_length is set to
4, each of the two tensors contains four token IDs. Note that an input size of 4 is quite
small and only chosen for simplicity. It is common to train LLMs with input sizes of at
least 256.
To understand the meaning of stride=1, let’s fetch another batch from this dataset:

    second_batch = next(data_iter)
    print(second_batch)
    
The second batch has the following contents:

    [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]

Exercise 2.2 Data loaders with different strides and context sizes
To develop more intuition for how the data loader works, try to run it with different
settings such as max_length=2 and stride=2, and max_length=8 and stride=2.

If we compare the first and second batches, we can see that the second batch’s token
IDs are shifted by one position (for example, the second ID in the first batch’s input is
367, which is the first ID of the second batch’s input). The stride setting dictates the
number of positions the inputs shift across batches, emulating a sliding window
approach, as demonstrated in figure 2.14.
Batch sizes of 1, such as we have sampled from the data loader so far, are useful for
illustration purposes. If you have previous experience with deep learning, you may
know that small batch sizes require less memory during training but lead to more noisy model updates.

Just like in regular deep learning, the batch size is a tradeoff and
a hyperparameter to experiment with when training LLMs.
Let’s look briefly at how we can use the data loader to sample with a batch size
greater than 1:

    dataloader = create_dataloader_v1(
    raw_text, batch_size=8, max_length=4, stride=4,
    shuffle=False
    )
    data_iter = iter(dataloader)
    inputs, targets = next(data_iter)
    print("Inputs:\n", inputs)
    print("\nTargets:\n", targets)
    
This prints
    
    Inputs:
    tensor([[ 40, 367, 2885, 1464],
    [ 1807, 3619, 402, 271],
    [10899, 2138, 257, 7026],
    [15632, 438, 2016, 257],
    [ 922, 5891, 1576, 438],
    [ 568, 340, 373, 645],
    [ 1049, 5975, 284, 502],
    [ 284, 3285, 326, 11]])]
    
    Targets:
    tensor([[ 367, 2885, 1464, 1807],
    [ 3619, 402, 271, 10899],
    [ 2138, 257, 7026, 15632],
    [ 438, 2016, 257, 922],
    [ 5891, 1576, 438, 568],
    [ 340, 373, 645, 1049],
    [ 5975, 284, 502, 284],
    [ 3285, 326, 11, 287]])

Note that we increase the stride to 4 to utilize the data set fully (we don’t skip a single
word). This avoids any overlap between the batches since more overlap could lead to
increased overfitting.

